5M: min,max,mode,mean,median
box plot:minimum, first quartile, median, third quartile, and maximum.
Interval quartiel:third quartile-first quartile
right skewed, left skewed, sysmetic(normal distribution)

Statistics and parameters are generally the mean or proportion for a group.
Statistics being the value for the sample. Parameters being the value for the population. 
The population is our entire group of interest, while a sample is the selected subset of the population.

+++++++++++++++++++sampling++++++++++++++++++++++++++++++++++++++++++++
A sampling distribution is the distribution of a statistic.

1.The sampling distribution is centered on the original parameter value. !!!!!!!!

2.The sampling distribution decreases its variance depending on the sample size used. 
Specifically, the variance of the sampling distribution is equal to the variance of the original data divided by the sample size used. 
This is always true for the variance of a sample mean!
p*(1-p)/n  is the theoretical variance of the sampling distribution for bionomial distribution.

In general, we say if we have a random variable, X, with variance of σ^2
 , then the distribution of which the sampling distribution of the sample mean, has a variance of (σ^2)/n
 
 
The Central Limit Theorem states that with large enough sample sizes our sample mean will follow a normal distribution, 
but it turns out this is true for more than just the sample mean.

The Law of Large Numbers states that as a sample size increases, the sample mean will get closer to the population mean.
In general, if our statistic is a "good" estimate of a parameter, it will approach our parameter with larger sample sizes.

bootstrapping:自助法是一种从给定训练集中有放回的均匀抽样
想要得到一个population的parameter，我们用statistics的方法去逼近。
statistics的方法是，首先我们有一个样本集合，从这个样本集合里面，我们需要采集N个样本（N要合理的多，大数定理），
然后利用bootstrapping方法，找到抽取样本的正太分布，抽取次数越多，样本的正态分布中心越趋近于population中的parameter
比如要研究喝咖啡人的概率,我们只有200 个人的数据，我们有放回的从这200人中抽N=100次，组成一个样本集合，然后重复10000次，
这样相当于我们有10000个的100个人的样本，然后去利用这个样本去做发现正态分布，这个正太分布的中心就接近于喝咖啡人的真实概率。
参看同文件夹下的confidence_interval 案例


+++++++++++++++++++confidence intervals+++++++++++++++++++++++++++++++
Using confidence intervals and hypothesis testing, you are able to provide statistical significance in making decisions.
However, it is also important to take into consideration practical significance in making decisions. 
Practical significance takes into consideration other factors of your situation that might not be considered directly in the results of your hypothesis test or confidence interval.
Constraints like space, time, or money are important in business decisions. 
However, they might not be accounted for directly in a statistical test.

It is important to understand the way that your sample size and confidence level relate to the confidence interval you achieve at the end of your analysis.

Assuming you control all other items of your analysis:

Increasing your sample size will decrease the width of your confidence interval.
Increasing your confidence level (say 95% to 99%) will increase the width of your confidence interval.
You saw that you can compute:

The confidence interval width as the difference between your upper and lower bounds of your confidence interval.
The margin of error is half the confidence interval width, 
and the value that you add and subtract from your sample estimate to achieve your confidence interval final results.

Confidence Intervals (& Hypothesis Testing) vs. Machine Learning
Confidence intervals take an aggregate approach towards the conclusions made based on data, 
as these tests are aimed at understanding population parameters (which are aggregate population values).

Alternatively, machine learning techniques take an individual approach towards making conclusions, 
as they attempt to predict an outcome for each specific data point.

++++++++++++++++++++++++++++++++hypothesis testing++++++++++++++++++++++++++++++++++++
H_0 is true before you collect any data.
H_0  usually states there is no effect or that two groups are equal.

The H_0 and H_1 are competing, non-overlapping hypotheses.
H1 is what we would like to prove to be true.
H_0 contains an equal sign of some kind - either =, <=, or >=.
H_1 contains the opposition of the null - either ≠, >>, or <<.

You saw that the statement, "Innocent until proven guilty" is one that suggests the following hypotheses are true:
H_0 Innocent
H_1 Guilty
We can relate this to the idea that "innocent" is true before we collect any data.
Then the alternative must be a competing, non-overlapping hypothesis. 
Hence, the alternative hypothesis is that an individual is guilty.
Before collecting any data, the null hypothesis is the hypothesis we believe to be true.

========================================================================================
Example:
we wanted to test if a new page was better than an existing page, 
we set that up in the alternative. Two indicators are that the null should hold the equality,
and the statement we would like to be true should be in the alternative. Therefore, it would look like this:
H_0: μ1≤μ2
H_1: μ1>μ2
Here μ1 represents the population mean return from the new page. 
Similarly, μ2 represents the population mean return from the old page.
========================================================================

Type I Errors
Type I errors have the following features:
You should set up your null and alternative hypotheses, so that the worse of your errors is the type I error.
They are denoted by the symbol α.
The definition of a type I error is: Deciding the alternative (H1) is true, when actually (H0) is true.
Type I errors are often called false positives.
Type II Errors
They are denoted by the symbol β.The definition of a type II error is: Deciding the null (H0) is true, 
when actually (H1) is true.
Type II errors are often called false negatives.
In the most extreme case, we can always choose one hypothesis (say always choosing the null)
to ensure that a particular error never occurs (never a type I error assuming we always choose the null).
However, more generally, there is a relationship where with a single set of data decreasing your chance of one type of error, 
increases the chance of the other error occurring.

=================================================================================
You are always performing hypothesis tests on population parameters, never on statistics. 
Statistics are values that you already have from the data, 
so it does not make sense to perform hypothesis tests on these values.

Common hypothesis tests include:
Testing a population mean (One sample t-test).
Testing the difference in means (Two sample t-test)
Testing the difference before and after some treatment on the same individual (Paired t-test)
Testing a population proportion (One sample z-test)
Testing the difference between population proportions (Two sample z-test)

You can use one of these sites to provide a t-table or z-table to support one of the above approaches:
t-table
t-table or z-table
There are literally hundreds of different hypothesis tests! 
However, instead of memorizing how to perform all of these tests, 
you can find the statistic(s) that best estimates the parameter(s) you want to estimate, 
you can bootstrap to simulate the sampling distribution. 
Then you can use your sampling distribution to assist in choosing the appropriate hypothesis.

========================P value understanding and example==================
The p-value is the probability of getting our statistic or a more extreme value if the null is true.
Therefore, small p-values suggest our null is not true. Rather, our statistic is likely to have come from a different distribution than the null.
When the p-value is large, we have evidence that our statistic was likely to come from the null hypothesis.
Therefore, we do not have evidence to reject the null.
By comparing our p-value to our type I error threshold (α), we can make our decision about which hypothesis we will choose.
pval≤α⇒ Reject H0
pval>α⇒ Fail to Reject H0	


P value画的方向应该与H1的方向一致，这样才能得出，P越大，越in favor of H0，越小，越reject H0
Example
H0:μ>=5
H1:μ<5
sample mean = 10, based on the sample mean,we would expect the population mean to be close to this.
Because 10 is to the right of the null mean of 5, and the alternative is less than. At 5,plot a normal distribution
At 10, we shade to the left.
The shaded area is obviously large. so P value is large. When P value is large, we should be in favor of H0.

Let's reverse the condition.
H0:μ<=5
H1:μ>5
sample mean = 10, we would expect the H0 should be rejected based on sample mean.
Check the P value, at 5,plot a normal distribution.
At 10, we shade to the right becase the alternative is greater than. P value is relatively small, we should reject H0

The word accept is one that is avoided when making statements regarding the null and alternative. 
You are not stating that one of the hypotheses is true. 
Rather, you are making a decision based on the likelihood of your data coming from the null hypothesis with regard to your type I error threshold.
Therefore, the wording used in conclusions of hypothesis testing includes:
We reject the null hypothesis or We fail to reject the null hypothesis. 
This lends itself to the idea that you start with the null hypothesis true by default, 
and "choosing" the null at the end of the test would have been the choice even if no data were collected.
=====================================================
One of the most important aspects of interpreting any statistical results 
(and one that is frequently overlooked) is assuring that your sample is truly representative of your population of interest.

Hypothesis Testing vs. Machine Learning
With large sample sizes, hypothesis testing leads to even the smallest of findings as statistically significant. 
However, these findings might not be practically significant at all.
For example, imagine you find that statistically more people prefer beverage 1 to beverage 2 in a study of more than one million people. 
Based on this you decide to open a shop to sell beverage 1. 
You then find out that beverage 1 is only more popular than beverage 2 by 0.0002% (but a statistically significant amount with your large sample size). 
Practically, maybe you should have opened a store that sold both.
Hypothesis testing takes an aggregate approach towards the conclusions made based on data, 
as these tests are aimed at understanding population parameters (which are aggregate population values).
Alternatively, machine learning techniques take an individual approach towards making conclusions, 
as they attempt to predict an outcome for each specific data point.
==============================================================================
When performing more than one hypothesis test, your type I error compounds.
In order to correct for this, a common technique is called the Bonferroni correction. 
This correction is very conservative, but says that your new type I error rate should be the error rate you actually want divided by the number of tests you are performing.
Therefore, if you would like to hold a type I error rate of 1% for each of 20 hypothesis tests, the Bonferroni corrected rate would be 0.01/20 = 0.0005. 
This would be the new rate you should use as your comparison to the p-value for each of the 20 tests to make your decision.
Other Techniques
Additional techniques to protect against compounding type I errors include:
 Tukey correction
 Q-values




