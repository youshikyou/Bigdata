 In these cases, you will be using both quantitative and categorical x-variables to predict a quantitative response. 
 That is, you will be creating equations that like this to predict your response:
 y^=b0+b1x1+b2x2+b3x3+b4x4
 Furthermore, you will learn about how to assess problems that can happen in multiple linear regression, 
 how to address these problems, and how to assess how well your model is performing.
 It turns out Rsquared can be used, but might be misleading. 
 And, unfortunately, the correlation coefficient is only a measure of the linear relationship
 between two quantitative variables, so it will not be very useful in the multiple linear regression case.
 Here is a wonderful free supplementary book: Introduction to Statistical Learning.
 This is an absolutely spectacular book for getting started with machine learning, 
 and Chapter 3 discusses many of the ideas in this lesson. 
 The programming performed in the text is in R, but here is an additional resource,
 not created by the book's authors, that provides Jupyter Notebooks in Python with notes
 and answers to nearly all the questions from the book:
 https://www.reddit.com/r/learnpython/comments/6rkh3u/introduction_to_statistical_learning_with_python/
 
 To grasp the linear algebra
 https://www.khanacademy.org/math/linear-algebra

++++++++++++++++How Do We Find the "Right" Coefficients in Multiple Linear Regression+++++++++++++
In the simple linear regression section, you saw how we were interested in minimizing the squared distance
between each actual data point and the predicted value from our model.

But in multiple linear regression, we are actually looking at points that live in not just a two dimensional space.
For a full derivation of how this works, this article provides a breakdown of the steps.
The takeaway for us is that we can find the optimal β estimates by calculating (X'X)^{-1}X'y
In the following video, you will use statsmodels to obtain the coefficients 
similar to how we did it in the last concept, but you will also solve for the coefficients 
using the equation above to show the results are not magic.

+++++++++++The Math Behind Dummy Variables++++++++++++++++
The categorical variables will be changed to dummy variables in order to be added to your linear models.
Then, you will need to drop one of the dummy columns in order to make your matrices full rank.
If you remember back to the closed form solution for the coefficients in regression, we have β is estimated by
(X'X)^{-1}X'y
In order to take the inverse of (X'X), the matrix X must be full rank. 
That is, all of the columns of X must be linearly independent.
If you do not drop one of the columns (from the model, not from the dataframe)
when creating the dummy variables, your solution is unstable and results from Python are unreliable. 
The takeaway is ... when you create dummy variables using 0, 1 encodings, 
you always need to drop one of the columns from the model to make sure your matrices are full rank 
(and that your solutions are reliable from Python).
The reason for this is linear algebra. Specifically, in order to invert matrices, 
a matrix must be full rank (that is, all the columns need to be linearly independent). 
Therefore, you need to drop one of the dummy columns, to create linearly independent columns (and a full rank matrix).
The number of dummy variables we need to add is always the number of levels of the categorical variable minus 1.
Example: if the categorical has a ratings scale of: 'great', 'good', 'okay', 'poor', or 'awful'. 
There should be a total of 5 coefficients,the intercept plus 4 dummy variables for the other variables. 
