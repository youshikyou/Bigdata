 In these cases, you will be using both quantitative and categorical x-variables to predict a quantitative response. 
 That is, you will be creating equations that like this to predict your response:
 y^=b0+b1x1+b2x2+b3x3+b4x4
 Furthermore, you will learn about how to assess problems that can happen in multiple linear regression, 
 how to address these problems, and how to assess how well your model is performing.
 It turns out Rsquared can be used, but might be misleading. Rsquared is the correlation efficient's square.
 And, unfortunately, the correlation coefficient is only a measure of the linear relationship
 between two quantitative variables, so it will not be very useful in the multiple linear regression case.
 Here is a wonderful free supplementary book: Introduction to Statistical Learning.
 This is an absolutely spectacular book for getting started with machine learning, 
 and Chapter 3 discusses many of the ideas in this lesson. 
 The programming performed in the text is in R, but here is an additional resource,
 not created by the book's authors, that provides Jupyter Notebooks in Python with notes
 and answers to nearly all the questions from the book:
 https://www.reddit.com/r/learnpython/comments/6rkh3u/introduction_to_statistical_learning_with_python/
 
 To grasp the linear algebra
 https://www.khanacademy.org/math/linear-algebra

++++++++++++++++How Do We Find the "Right" Coefficients in Multiple Linear Regression+++++++++++++
In the simple linear regression section, you saw how we were interested in minimizing the squared distance
between each actual data point and the predicted value from our model.

But in multiple linear regression, we are actually looking at points that live in not just a two dimensional space.
For a full derivation of how this works, this article provides a breakdown of the steps.
The takeaway for us is that we can find the optimal β estimates by calculating (X'X)^{-1}X'y
In the following video, you will use statsmodels to obtain the coefficients 
similar to how we did it in the last concept, but you will also solve for the coefficients 
using the equation above to show the results are not magic.

+++++++++++The Math Behind Dummy Variables++++++++++++++++
The categorical variables will be changed to dummy variables in order to be added to your linear models.
Then, you will need to drop one of the dummy columns in order to make your matrices full rank.
If you remember back to the closed form solution for the coefficients in regression, we have β is estimated by
(X'X)^{-1}X'y
In order to take the inverse of (X'X), the matrix X must be full rank. 
That is, all of the columns of X must be linearly independent.
If you do not drop one of the columns (from the model, not from the dataframe)
when creating the dummy variables, your solution is unstable and results from Python are unreliable. 
The takeaway is ... when you create dummy variables using 0, 1 encodings, 
you always need to drop one of the columns from the model to make sure your matrices are full rank 
(and that your solutions are reliable from Python).
The reason for this is linear algebra. Specifically, in order to invert matrices, 
a matrix must be full rank (that is, all the columns need to be linearly independent). 
Therefore, you need to drop one of the dummy columns, to create linearly independent columns (and a full rank matrix).
The number of dummy variables we need to add is always the number of levels of the categorical variable minus 1.
Example: if the categorical has a ratings scale of: 'great', 'good', 'okay', 'poor', or 'awful'. 
There should be a total of 5 coefficients,the intercept plus 4 dummy variables for the other variables. 
Which of the below are true regarding the dummy variables we add to our multiple linear regression models?
Let X be the X matrix as defined in the previous Screencast:

   -There should always be as many dummy variables added to your X matrix 
     as the number of levels of each categorical variable minus 1.
   -The reason for dropping a dummy variable is to assure that all of our columns are linearly independent.
   -The reason for dropping a dummy variable is to assure that the dot product of X'X is invertible.
   -The reason for dropping a dummy variable is to assure that your X matrix is full rank.
   
Notice that each of your comparisons are to the baseline category for the categorical variables.
Also, each quantitative variable is interpreted as the expected change in the response for a one 
unit increase in the quantitative variable.


===========================Model Assumptions And How To Address Each
Here are the five potential problems related to Multiple Linear Regression that we mentioned in the previous video, 
that are addressed in Introduction to Statistical Learning:
   -Non-linearity of the response-predictor relationships
   -Correlation of error terms
   -Non-constant Variance and Normally Distributed Errors
   -Outliers/ High leverage points
   -Multicollinearity
This text is a summary of how to identify whether these problems exist, as well as how to address them. 
This is a common interview question asked by statisticians, but its practical importance is 
hit or miss depending on the purpose of your model. 
In the upcoming concepts, we will look more closely at specific points that I believe deserve more attention, 
but below you will see a more exhaustive introduction to each topic. Let's take a closer look at each of these items.

-Linearity
The assumption of linearity is that a linear model is the relationship that truly exists 
between your response and predictor variables. If this isn't true, then your predictions will not be very accurate. 
Additionally, the linear relationships associated with your coefficients really aren't useful either.

In order to assess if a linear relationship is reasonable, a plot of the residuals (y - \hat{y}) by the predicted values
(\hat{y}) is often useful. If there are curvature patterns in this plot, it suggests that a linear model might not 
actually fit the data, and some other relationship exists between the predictor variables and response. 
There are many ways to create non-linear models (even using the linear model form)
In the image at the bottom of this page, these are considered the biased models. 
Ideally, we want to see a random scatter of points like the top left residual plot in the image.

-Correlated Errors
Correlated errors frequently occur when our data are collected over time 
(like in forecasting stock prices or interest rates in the future) or data are spatially related 
(like predicting flood or drought regions). We can often improve our predictions by using information 
from the past data points (for time) or the points nearby (for space).
The main problem with not accounting for correlated errors is that you can often use this correlation 
to your advantage to better predict future events or events spatially close to one another.
One of the most common ways to identify if you have correlated errors is based on the domain 
from which the data were collected. If you are unsure, there is a test known as a Durbin-Watson test 
that is commonly used to assess whether correlation of the errors is an issue. 
Then ARIMA or ARMA models are commonly implemented to use this correlation to make better predictions.

-Non-constant Variance and Normally Distributed Errors
Non-constant variance is when the spread of your predicted values differs depending on which value 
you are trying to predict. This isn't a huge problem in terms of predicting well. 
However, it does lead to confidence intervals and p-values that are inaccurate. 
Confidence intervals for the coefficients will be too wide for areas where the actual values are closer 
to the predicted values, but too narrow for areas where the actual values are more spread out from the predicted values.
Commonly, a log (or some other transformation of the response variable is done) 
in order to "get rid" of the non-constant variance. In order to choose the transformation,
a Box-Cox(https://www.statisticshowto.com/box-cox-transformation/) is commonly used.
Non-constant variance can be assessed again using a plot of the residuals by the predicted values. 
In the image at the bottom of the page, non-constant variance is labeled as heteroscedastic. 
Ideally, we want an unbiased model with homoscedastic residuals (consistent across the range of values).
Though the text does not discuss normality of the residuals, this is an important assumption of regression 
if you are interested in creating reliable confidence intervals. More on this topic is provided (https://www.itl.nist.gov/div898/handbook/pri/section2/pri24.htm).

-Outliers/Leverage Points
Outliers and leverage points are points that lie far away from the regular trends of your data. 
These points can have a large influence on your solution. In practice, these points might even be typos. 
If you are aggregating data from multiple sources, it is possible that some of the data values were carried 
over incorrectly or aggregated incorrectly.
Other times outliers are accurate and true data points, not necessarily measurement or data entry errors. 
In these cases, 'fixing' is more subjective. Often the strategy for working with these points is dependent on 
the goal of your analysis. Linear models using ordinary least squares, in particular, are not very robust. 
That is, large outliers may greatly change our results. There are techniques to combat this - largely known as 
regularization techniques. These are beyond the scope of this class, but they are quickly discussed in the 
free course on machine learning.

-Multi-collinearity
Multicollinearity is when we have predictor variables that are correlated with one another. 
One of the main concerns of multicollinearity is that it can lead to coefficients being flipped from the direction 
we expect from simple linear regression.
One of the most common ways to identify multicollinearity is with bivariate plots or with variance inflation factors 
(or VIFs). https://etav.github.io/python/vif_factor_python.html

We would like x-variables to be related to the response, but not to be related to one another.
When our x-variables are correlated with one another, this is known as multicollinearity. 
Multicollinearity has two potential negative impacts. As you saw in the previous example,

1. The expected relationships between your x-variables and the response may not hold when multicollinearity is present. 
That is, you may expect a positive relationship between the explanatory variables and the response
(based on the bivariate relationships), but in the multiple linear regression case,
it turns out the relationship is negative.
Example: area, bedroom,bathroom in house price
using seaborn
seaborn.pairplot(df[['area','bedroom','bathroom']]). 
We can see the strong positive relationship among these variables.
But in the model, the bedroom has negative coefficient associated with it.

2. Our hypothesis testing results may not be reliable. It turns out that having correlated explanatory variables 
means that our coefficient estimates are less stable. That is, standard deviations (often called standard errors)
associated with your regression coefficients are quite large. Therefore, a particular variable might be useful 
for predicting the response, but because of the relationship it has with other x-variables, 
you will no longer see this association.

We have also looked at two different ways of identifying multicollinearity:
  1.Looking at the correlation of each explanatory variable with each other explanatory variable
  (with a plot or the correlation coefficient).
  2.Looking at VIFs for each variable.
  
When VIFs are greater than 10, this suggests that multicollinearity is certainly a problem in your model. 
Some experts even suggest that VIFs of greater than 5 can be problematic. 
In most cases, not just one VIF is high, but rather many VIFs are high, 
as these are measures of how related variables are with one another.
The most common way of working with correlated explanatory variables in a multiple linear regression model
is simply to remove one of the variables that is most related to the other variables. 
Choosing an explanatory variable that you aren't interested in, or isn't as important to you, is a common choice.

VIF_i = 1/(1-Ri^2), 
x_i = b0+b1x1+b2x2...+bnxn
all other x variables (excluding x_i)are used to predict then compute Ri^2. 
It is used to know how x_i can be predicted by other x varialbes. 
Ri is higher, the xi can be predicted by other variables,then VIF_i is higher. 


