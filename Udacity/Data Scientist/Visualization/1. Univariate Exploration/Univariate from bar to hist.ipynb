{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pie chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for the pie chart seen above\n",
    "sorted_counts = df['cat_var'].value_counts()\n",
    "plt.pie(sorted_counts, labels = sorted_counts.index, startangle = 90,\n",
    "        counterclock = False);\n",
    "plt.axis('square')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The axis function call and 'square' argument makes it so that the scaling of the plot is equal on both the x- and y-axes. Without this call, the pie could end up looking oval-shaped, rather than a circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#donuts chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_counts = df['cat_var'].value_counts()\n",
    "plt.pie(sorted_counts, labels = sorted_counts.index, startangle = 90,\n",
    "        counterclock = False, wedgeprops = {'width' : 0.4});\n",
    "plt.axis('square')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histograms\n",
    "plt.hist(data = df, x = 'num_var')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the hist function divides the data into 10 bins, based on the range of values taken. In almost every case, we will want to change these settings. Usually, having only ten bins is too few to really understand the distribution of the data. And the default tick marks are often not on nice, 'round' values that make the ranges taken by each bin easy to interpret. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = np.arange(0, df['num_var'].max()+1, 1)\n",
    "plt.hist(data = df, x = 'num_var', bins = bin_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating histograms, it's useful to play around with different bin widths to see what represents the data best. Too many bins, and you may see too much noise that interferes with identification of the underlying signal. Too few bins, and you may not be able to see the true signal in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [10, 5]) # larger figure size for subplots\n",
    "\n",
    "# histogram on left, example of too-large bin size\n",
    "plt.subplot(1, 2, 1) # 1 row, 2 cols, subplot 1\n",
    "bin_edges = np.arange(0, df['num_var'].max()+4, 4)\n",
    "plt.hist(data = df, x = 'num_var', bins = bin_edges)\n",
    "\n",
    "# histogram on right, example of too-small bin size\n",
    "plt.subplot(1, 2, 2) # 1 row, 2 cols, subplot 2\n",
    "bin_edges = np.arange(0, df['num_var'].max()+1/4, 1/4)\n",
    "plt.hist(data = df, x = 'num_var', bins = bin_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example puts two plots side by side through use of the subplot function, whose arguments specify the number of rows, columns, and index of the active subplot (in that order). The figure() function is called with the \"figsize\" parameter so that we can have a larger figure to support having multiple subplots. (More details on figures and subplots are coming up next in the lesson.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seaborn function distplot can also be used to plot a histogram, and is integrated with other univariate plotting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.distplot(df['num_var'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we specify the data to be plotted, note that the first argument must be the Series or array with the points to be plotted. This is in contrast to our ability to specify a data source and column as separate arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distplot function has built-in rules for specifying histogram bins, and by default plots a curve depicting the kernel density estimate (KDE) on top of the data. The vertical axis is based on the KDE, rather than the histogram: you shouldn't expect the total heights of the bars to equal 1, but the area under the curve should equal 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = np.arange(0, df['num_var'].max()+1, 1)\n",
    "sb.distplot(df['num_var'], bins = bin_edges, kde = False,\n",
    "            hist_kws = {'alpha' : 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alpha (transparency) setting must be associated as a dictionary to \"hist_kws\" since there are other underlying plotting functions, like the KDE, that have their own optional keyword parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisite package imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from solutions_univ import histogram_solution_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon = pd.read_csv('./data/pokemon.csv')\n",
    "pokemon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon = pd.read_csv('./data/pokemon.csv')\n",
    "bins = np.arange(20, pokemon['special-defense'].max()+5, 5)\n",
    "plt.hist(pokemon['special-defense'], bins = bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figures, Axes, and Subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([.125, .125, .775, .755])\n",
    "ax.hist(data = df, x = 'num_var')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "figure() creates a new Figure object, a reference to which has been stored in the variable fig. One of the Figure methods is .add_axes(), which creates a new Axes object in the Figure. The method requires one list as argument specifying the dimensions of the Axes: the first two elements of the list indicate the position of the lower-left hand corner of the Axes (in this case one quarter of the way from the lower-left corner of the Figure) and the last two elements specifying the Axes width and height, respectively. We refer to the Axes in the variable ax. Finally, we use the Axes method .hist() just like we did before with plt.hist()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Axes objects with seaborn, seaborn functions usually have an \"ax\" parameter to specify upon which Axes a plot will be drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([.125, .125, .775, .755])\n",
    "base_color = sb.color_palette()[0]\n",
    "sb.countplot(data = df, x = 'cat_var', color = base_color, ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [10, 5]) # larger figure size for subplots\n",
    "\n",
    "# example of somewhat too-large bin size\n",
    "plt.subplot(1, 2, 1) # 1 row, 2 cols, subplot 1\n",
    "bin_edges = np.arange(0, df['num_var'].max()+4, 4)\n",
    "plt.hist(data = df, x = 'num_var', bins = bin_edges)\n",
    "\n",
    "# example of somewhat too-small bin size\n",
    "plt.subplot(1, 2, 2) # 1 row, 2 cols, subplot 2\n",
    "bin_edges = np.arange(0, df['num_var'].max()+1/4, 1/4)\n",
    "plt.hist(data = df, x = 'num_var', bins = bin_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, plt.figure(figsize = [10, 5])creates a new Figure, with the \"figsize\" argument setting the width and height of the overall figure to 10 inches by 5 inches, respectively. Even if we don't assign any variable to return the function's output, Python will still implicitly know that further plotting calls that need a Figure will refer to that Figure as the active one.\n",
    "\n",
    "Then, plt.subplot(1, 2, 1) creates a new Axes in our Figure, its size determined by the subplot() function arguments. The first two arguments says to divide the figure into one row and two columns, and the third argument says to create a new Axes in the first slot. Slots are numbered from left to right in rows from top to bottom. Note in particular that the index numbers start at 1 (rather than the usual Python indexing starting from 0). (You'll see the indexing a little better in the example at the end of the page.) Again, Python will implicitly set that Axes as the current Axes, so when the plt.hist() call comes, the histogram is plotted in the left-side subplot.\n",
    "\n",
    "Finally, plt.subplot(1, 2, 2) creates a new Axes in the second subplot slot, and sets that one as the current Axes. Thus, when the next plt.hist() call comes, the histogram gets drawn in the right-side subplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 4) # grid of 3x4 subplots\n",
    "axes = axes.flatten() # reshape from 3x4 array into 12-element vector\n",
    "for i in range(12):\n",
    "    plt.sca(axes[i]) # set the current Axes\n",
    "    plt.text(0.5, 0.5, i+1) # print conventional subplot index number to middle of Axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a special note for the text, the Axes limits are [0,1] on each Axes by default, and we increment the iterator counter i by 1 to get the subplot index, if we were creating the subplots through subplot()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing a Plot for Discrete Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to plot a discrete quantitative variable, it is possible to select either a histogram or a bar chart to depict the data.\n",
    "The histogram is the most immediate choice since the data is numeric, but there's one particular consideration to make regarding the bin edges. Since data points fall on set values, it can help to reduce ambiguity by putting bin edges between the actual values taken by the data. Your readers may not know that values on bin edges end up in the bin to their right, so this can help remove potential confusion when they interpret the plot. Compare the two visualizations of 100 random die rolls below (in die_rolls), with bin edges _on_ the values in the left subplot, and bin edges in between values in the right subplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a09ae4961fd8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# histogram on the left, bin edges on integers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbin_edges\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# note `+1.1`, see below\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize = [10, 5])\n",
    "\n",
    "# histogram on the left, bin edges on integers\n",
    "plt.subplot(1, 2, 1)\n",
    "bin_edges = np.arange(2, 12+1.1, 1) # note `+1.1`, see below\n",
    "plt.hist(die_rolls, bins = bin_edges)\n",
    "plt.xticks(np.arange(2, 12+1, 1))\n",
    "\n",
    "# histogram on the right, bin edges between integers\n",
    "plt.subplot(1, 2, 2)\n",
    "bin_edges = np.arange(1.5, 12.5+1, 1)\n",
    "plt.hist(die_rolls, bins = bin_edges)\n",
    "plt.xticks(np.arange(2, 12+1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice for the left histogram, in a deviation from the examples that have come before, I've added 1.1 to the max value (12) for setting the bin edges, rather than just the desired bin width of 1. Recall that data that is equal to the rightmost bin edge gets lumped in to the last bin. This presents a potential problem in perception when a lot of data points take the maximum value, and so is especially relevant when the data takes on discrete values. The 1.1 adds an additional bin to the end to store the die rolls of value 12 alone, to avoid having the last bar catch both 11 and 12.\n",
    "\n",
    "Alternatively to the histogram, consider if a bar chart with non-connected bins might serve your purposes better. The plot below takes the code from before, but adds the \"rwidth\" parameter to set the proportion of the bin widths that will be filled by each histogram bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = np.arange(1.5, 12.5+1, 1)\n",
    "plt.hist(die_rolls, bins = bin_edges, rwidth = 0.7)\n",
    "plt.xticks(np.arange(2, 12+1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With \"rwidth\" set to 0.7, the bars will take up 70% of the space allocated by each bin, with 30% of the space left empty. This changes the default display of the histogram (which you could think of as \"rwidth = 1\") into a bar chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding gaps between bars, you emphasize the fact that the data is discrete in value. On the other hand, plotting your quantitative data in this manner might cause it to be interpreted as ordinal-type data, which can have an effect on overall perception.\n",
    "\n",
    "For continuous numeric data, you should not make use of the \"rwidth\" parameter, since the gaps imply discreteness of value. As another caution, it might be tempting to use seaborn's countplot function to plot the distribution of a discrete numeric variable as bars. Be careful about doing this, since each unique numeric value will get a bar, regardless of the spacing in values between bars. (For example, if the unique values were {1, 2, 4, 5}, missing 3, countplot would only plot four bars, with the bars for 2 and 4 right next to one another.) Also, even if your data is technically discrete numeric, you should probably not consider either of the variants depicted on this page unless the number of unique values is small enough to allow for the half-unit shift or discrete bars to be interpretable. If you have a large number of unique values over a large enough range, it's better to stick with the standard histogram than risk interpretability issues.\n",
    "\n",
    "While you might justify plotting discrete numeric data using a bar chart, you’ll be less apt to justify the opposite: plotting ordinal data as a histogram. The space between bars in a bar chart helps to remind the reader that values are not contiguous in an ‘interval’-type fashion: only that there is an order in levels. With that space removed as in a histogram, it's harder to remember this important bit of interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive Statistics, Outliers, and Axis Limits\n",
    "As you create your plots and perform your exploration, make sure that you pay attention to what the plots tell you that go beyond just the basic descriptive statistics. Note any aspects of the data like number of modes and skew, and note the presence of outliers in the data for further investigation.\n",
    "\n",
    "Related to the latter point, you might need to change the limits or scale of what is plotted to take a closer look at the underlying patterns in the data. This page covers the topic of axis limits; the next the topic of scales and transformations. In order to change a histogram's axis limits, you can add a Matplotlib xlim call to your code. The function takes a tuple of two numbers specifying the upper and lower bounds of the x-axis range. Alternatively, the xlim function can be called with two numeric arguments to the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [10, 5])\n",
    "\n",
    "# histogram on left: full data\n",
    "plt.subplot(1, 2, 1)\n",
    "bin_edges = np.arange(0, df['skew_var'].max()+2.5, 2.5)\n",
    "plt.hist(data = df, x = 'skew_var', bins = bin_edges)\n",
    "\n",
    "# histogram on right: focus in on bulk of data < 35\n",
    "plt.subplot(1, 2, 2)\n",
    "bin_edges = np.arange(0, 35+1, 1)\n",
    "plt.hist(data = df, x = 'skew_var', bins = bin_edges)\n",
    "plt.xlim(0, 35) # could also be called as plt.xlim((0, 35))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the generic example above, we might be interested in comparing patterns in other variables between data points that take values less than 35 to those that take values greater than 35. For anything that is concentrated on the bulk of the data in the former group (< 35), use of axis limits can allow focusing on data points in that range without needing to go through creation of a new DataFrame filtering out the data points in the latter group (> 35)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scales and Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain data distributions will find themselves amenable to scale transformations. The most common example of this is data that follows an approximately log-normal distribution. This is data that, in their natural units, can look highly skewed: lots of points with low values, with a very long tail of data points with large values. However, after applying a logarithmic transform to the data, the data will follow a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [10, 5])\n",
    "\n",
    "# left histogram: data plotted in natural units\n",
    "plt.subplot(1, 2, 1)\n",
    "bin_edges = np.arange(0, data.max()+100, 100)\n",
    "plt.hist(data, bins = bin_edges)\n",
    "plt.xlabel('values')\n",
    "\n",
    "# right histogram: data plotted after direct log transformation\n",
    "plt.subplot(1, 2, 2)\n",
    "log_data = np.log10(data) # direct data transform\n",
    "log_bin_edges = np.arange(0.8, log_data.max()+0.1, 0.1)\n",
    "plt.hist(log_data, bins = log_bin_edges)\n",
    "plt.xlabel('log(values)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot on the left, the few data points with value above 1000 mash the majority of the points into the bins on the far left. With the plot on the right, the logarithmic transform makes those large points look in line with the rest: a raw value of 1000 becomes a value of 3 under log transform, and a raw value of 100 becomes a log-transformed value of 2. The big problem with the right-side plot is that the units on the x-axis are difficult to interpret: for most people, it is only easy to convert from log values to natural values on the integers (and this assumes a nice base like 10 as used in the example).\n",
    "\n",
    "This is where scale transformations are handy. In a scale transformation, the gaps between values are based on the transformed scale, but you can interpret data in the variable's natural units. It is also a convenient approach since you won't need to engineer new features. Matplotlib's xscale function includes a few built-in transformations: we'll use the 'log' scale here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = np.arange(0, data.max()+100, 100)\n",
    "plt.hist(data, bins = bin_edges)\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice two things about the plot now. Even though the data is on a log scale, the bins are still linearly spaced. This means that they change size from wide on the left to thin on the right, as the values increase multiplicatively. Secondly, the default label settings are still somewhat tricky to interpret, and are sparse as well.\n",
    "\n",
    "To address the bin size issue, we just need to change them so that they are evenly-spaced powers of 10. Depending on what you are plotting, a different base power like 2 might be useful instead. For the ticks, we can use xticks to specify locations and labels in their natural units. Remember: we aren't changing the values taken by the data, only how they're displayed. Between integer powers of 10, we don't have clean values for even markings, but we can still get close. Setting ticks in cycles of 1-3-10 or 1-2-5-10 are very useful for base-10 log transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = 10 ** np.arange(0.8, np.log10(data.max())+0.1, 0.1)\n",
    "plt.hist(data, bins = bin_edges)\n",
    "plt.xscale('log')\n",
    "tick_locs = [10, 30, 100, 300, 1000, 3000]\n",
    "plt.xticks(tick_locs, tick_locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important that the xticks are specified after xscale since that function has its own built-in tick settings.\n",
    "We've ended up with the same plot as when we performed the direct log transform, but now with a much nicer set of tick marks and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative Approach\n",
    "Be aware that a logarithmic transformation is not the only one possible. When we perform a logarithmic transformation, our data values have to all be positive; it's impossible to take a log of zero or a negative number. In addition, the transformation implies that additive steps on the log scale will result in multiplicative changes in the natural scale, an important implication when it comes to data modeling. The type of transformation that you choose may be informed by the context for the data. For example, this Wikipedia section provides a few examples of places where log-normal distributions have been observed.\n",
    "\n",
    "If you want to use a different transformation that's not available in xscale, then you'll have to perform some feature engineering. In cases like this, we want to be systematic by writing a function that applies both the transformation and its inverse. The inverse will be useful in cases where we specify values in their transformed units and need to get the natural units back. For the purposes of demonstration, let's say that we want to try plotting the above data on a square-root transformation. (Perhaps the numbers represent areas, and we think it makes sense to model the data on a rough estimation of radius, length, or some other 1-d dimension.) We can create a visualization on this transformed scale like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqrt_trans(x, inverse = False):\n",
    "    \"\"\" transformation helper function \"\"\"\n",
    "    if not inverse:\n",
    "        return np.sqrt(x)\n",
    "    else:\n",
    "        return x ** 2\n",
    "\n",
    "bin_edges = np.arange(0, sqrt_trans(data.max())+1, 1)\n",
    "plt.hist(data.apply(sqrt_trans), bins = bin_edges)\n",
    "tick_locs = np.arange(0, sqrt_trans(data.max())+10, 10)\n",
    "plt.xticks(tick_locs, sqrt_trans(tick_locs, inverse = True).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that data is a pandas Series, so we can use the apply method for the function. If it were a NumPy Array, we would need to apply the function like in the other cases. The tick locations could have also been specified with the natural values, where we apply the standard transformation function on the first argument of xticks instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scales and Transformation Practical excercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisite package imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from solutions_univ import scales_solution_1, scales_solution_2\n",
    "pokemon = pd.read_csv('./data/pokemon.csv')\n",
    "pokemon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: There are also variables in the dataset that don't have anything to do with the game mechanics, and are just there for flavor. Try plotting the distribution of Pokémon heights (given in meters). For this exercise, experiment with different axis limits as well as bin widths to see what gives the clearest view of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(0, pokemon['height'].max()+0.2, 0.2)\n",
    "plt.hist(data = pokemon, x = 'height', bins = bins)\n",
    "plt.xlim((0,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: In this task, you should plot the distribution of Pokémon weights (given in kilograms). Due to the very large range of values taken, you will probably want to perform an axis transformation as part of your visualization workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 10 ** np.arange(-1, 3.0+0.1, 0.1)\n",
    "ticks = [0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000]\n",
    "labels = ['{}'.format(val) for val in ticks]\n",
    "\n",
    "plt.hist(data = pokemon, x = 'weight', bins = bins)\n",
    "plt.xscale('log')\n",
    "plt.xticks(ticks, labels)\n",
    "plt.xlabel('Weight (kg)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel Density Estimation\n",
    "Earlier in this lesson, you saw an example of kernel density estimation (KDE) through the use of seaborn's distplot function, which plots a KDE on top of a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.distplot(df['num_var'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel density estimation is one way of estimating the probability density function of a variable. In a KDE plot, you can think of each observation as replaced by a small ‘lump’ of area. Stacking these lumps all together produces the final density curve. The default settings use a normal-distribution kernel, but most software that can produce KDE plots also include other kernel function options.\n",
    "\n",
    "Seaborn's distplot function calls another function, kdeplot, to generate the KDE. The demonstration code below also uses a third function called by distplot for illustration, rugplot. In a rugplot, data points are depicted as dashes on a number line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [0.0, 3.0, 4.5, 8.0]\n",
    "plt.figure(figsize = [12, 5])\n",
    "\n",
    "# left plot: showing kde lumps with the default settings\n",
    "plt.subplot(1, 3, 1)\n",
    "sb.distplot(data, hist = False, rug = True, rug_kws = {'color' : 'r'})\n",
    "\n",
    "# central plot: kde with narrow bandwidth to show individual probability lumps\n",
    "plt.subplot(1, 3, 2)\n",
    "sb.distplot(data, hist = False, rug = True, rug_kws = {'color' : 'r'},\n",
    "            kde_kws = {'bw' : 1})\n",
    "\n",
    "# right plot: choosing a different, triangular kernel function (lump shape)\n",
    "plt.subplot(1, 3, 3)\n",
    "sb.distplot(data, hist = False, rug = True, rug_kws = {'color' : 'r'},\n",
    "            kde_kws = {'bw' : 1.5, 'kernel' : 'tri'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting proportions from this plot type is slightly trickier than a standard histogram: the vertical axis indicates a density of data rather than straightforward proportions. Under a KDE plot, the total area between the 0-line and the curve will be 1. The probability of an outcome falling between two values is found by computing the area under the curve that falls between those values. Making area judgments like this without computer assistance is difficult and likely to be inaccurate.\n",
    "\n",
    "Despite the fact that making specific probability judgments are not as intuitive with KDE plots as histograms, there are still reasons to use kernel density estimation. If there are relatively few data points available, KDE provides a smooth estimate of the overall distribution of data. These ideas may not be so easily conveyed through histograms, in which the large discreteness of jumps may end up misleading.\n",
    "\n",
    "It should also be noted that there is a bandwidth parameter in KDE that specifies how wide the density lumps are. Similar to bin width for histograms, we need to choose a bandwidth size that best shows the signal in the data. A too-small bandwidth can make the data look noisier than it really is, and a too-large bandwidth can smooth out useful features that we could use to make inferences about the data. It’s good to keep this in mind in case the default bandwidths chosen by your visualization software don’t look quite right or if you need to perform further investigations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waffle Plots\n",
    "One alternative univariate plot type that you might see for categorical data is the waffle plot, also known as the square pie chart. While the standard pie chart uses a circle to represent the whole, a waffle plot is plotted onto a square divided into a 10x10 grid. Each small square in the grid represents one percent of the data, and a number of squares are colored by category to indicate total proportions. Compared to a pie chart, it is much easier to make precise assessments of relative frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no built-in function for waffle plots in Matplotlib or Seaborn, so we'll need to take some additional steps in order to build one with the tools available. First, we need to create a function to decide how many blocks to allocate to each category. The function below, percentage_blocks, uses a rule where each category gets a number of blocks equal to the number of full percentage points it covers. The remaining blocks to get to the full one hundred are assigned to the categories with the largest fractional parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_blocks(df, var):\n",
    "    \"\"\"\n",
    "    Take as input a dataframe and variable, and return a Pandas series with\n",
    "    approximate percentage values for filling out a waffle plot.\n",
    "    \"\"\"\n",
    "    # compute base quotas\n",
    "    percentages = 100 * df[var].value_counts() / df.shape[0]\n",
    "    counts = np.floor(percentages).astype(int) # integer part = minimum quota\n",
    "    decimal = (percentages - counts).sort_values(ascending = False)\n",
    "\n",
    "    # add in additional counts to reach 100\n",
    "    rem = 100 - counts.sum()\n",
    "    for cat in decimal.index[:rem]:\n",
    "        counts[cat] += 1\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to actually plot those counts as boxes in the waffle plot form. To do this, we'll make use of Matplotlib's bar function. We could have used this function earlier in the lesson to create our bar charts instead of Seaborn's countplot, but it would have required us to aggregate the data first to get the height of each bar. For the case of the waffle plot, we're going to specify the x- and y- coordinates of the boxes, and set their widths and heights to be equal, to create squares. The initial plotting code looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waffle_counts = percentage_blocks(df, 'cat_var')\n",
    "\n",
    "prev_count = 0\n",
    "# for each category,\n",
    "for cat in range(waffle_counts.shape[0]):\n",
    "    # get the block indices\n",
    "    blocks = np.arange(prev_count, prev_count + waffle_counts[cat])\n",
    "    # and put a block at each index's location\n",
    "    x = blocks % 10 # use mod operation to get ones digit\n",
    "    y = blocks // 10 # use floor division to get tens digit\n",
    "    plt.bar(x = x, height = 0.8, width = 0.8, bottom = y)\n",
    "    prev_count += waffle_counts[cat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blocks are drawn from left to right, bottom to top, using the ones and tens digits for numbers from 0 to 99 to specify the x- and y- positions, respectively. A loop is used to call the bar function once for each category; each time it is called, the plotted bars are assigned a different color.\n",
    "The last steps that we need to do involve aesthetic cleaning to polish it up for interpretability. We can take away the plot border and ticks, since they're arbitrary, but we should change the limits so that the boxes are square. We should also add a legend so that the mapping from colors to category levels is clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waffle_counts = percentage_blocks(df, 'cat_var')\n",
    "\n",
    "prev_count = 0\n",
    "# for each category,\n",
    "for cat in range(waffle_counts.shape[0]):\n",
    "    # get the block indices\n",
    "    blocks = np.arange(prev_count, prev_count + waffle_counts[cat])\n",
    "    # and put a block at each index's location\n",
    "    x = blocks % 10 # use mod operation to get ones digit\n",
    "    y = blocks // 10 # use floor division to get tens digit\n",
    "    plt.bar(x = x, height = 0.8, width = 0.8, bottom = y)\n",
    "    prev_count += waffle_counts[cat]\n",
    "\n",
    "# aesthetic wrangling\n",
    "plt.legend(waffle_counts.index, bbox_to_anchor = (1, 0.5), loc = 6)\n",
    "plt.axis('off')\n",
    "plt.axis('square'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two calls to Matplotlib's axis function make use of two convenience strings for arguments: 'off' removes the axis lines, ticks, and labels, while 'square' ensures that the scaling on each axis is equal within a square bounding box. As for the legend call, the first argument is a list of categories as obtained from the sorted waffle_counts Series variable. This will match each category to each bar call, in order. The \"bbox_to_anchor\" argument sets an anchor for the legend to the right side of the plot, and \"loc = 6\" positions the anchor to the center left of the legend. The final plot is as it looks at the top of the page:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other variants of the waffle plot exist to extend it beyond just displaying probabilities. By associating each square with an amount rather than a percentage, we can use waffle plots to show absolute frequencies instead. This might cause us to end up with something other than 100 squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each box represents five full counts\n",
    "waffle_counts = (df['cat_var'].value_counts() / 5).astype(int)\n",
    "\n",
    "prev_count = 0\n",
    "# for each category,\n",
    "for cat in range(waffle_counts.shape[0]):\n",
    "    # get the block indices\n",
    "    blocks = np.arange(prev_count, prev_count + waffle_counts[cat])\n",
    "    # and put a block at each index's location\n",
    "    x = blocks % 10\n",
    "    y = blocks // 10\n",
    "    plt.bar(y, 0.8, 0.8, x)\n",
    "    prev_count += waffle_counts[cat]\n",
    "\n",
    "# box size legend\n",
    "plt.bar(7.5, 0.8, 0.8, 2, color = 'white', edgecolor = 'black', lw = 2)\n",
    "plt.text(8.1, 2.4,'= 5 data points', va = 'center')\n",
    "\n",
    "# aesthetic wrangling\n",
    "plt.legend(waffle_counts.index, bbox_to_anchor = (0.8, 0.5), loc = 6)\n",
    "plt.axis('off')\n",
    "plt.axis('square')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, waffle_counts has been adjusted so that each box represents 5 data points. Most of the code is the same as before, though it should be noted that the x and y variables have been swapped in the bar function so that the boxes are plotted in columns from left to right. Additional bar and text calls have been added to the plot to act as an ad hoc legend. The positions of these elements, and the legend, have been adjusted manually through some trial and error to improve the aesthetic appeal. Note that this constitutes more of an explanatory polishing than it is a part of exploration!As a further extension, there's no restriction against us using icons for each tally, rather than just squares. Infographics often take this approach, by having each icon represent some number of units (e.g. one person icon representing one million people). But while it can be tempting to use icons to represent values as a bit of visual flair, an icon-based plot contains more chart junk than a bar chart that conveys the same information. There’s a larger cognitive challenge in having to count a number of icons to understand the scale of a value, compared to just referencing a box's endpoint on a labeled axis.\n",
    "\n",
    "One other downside of the waffle plot is that it is not commonly supported out of the box for most visualization libraries, including Matplotlib and Seaborn. The length of the demonstration code presented above is a testament to that. The effort required to create a meaningful and useful waffle plot means that it is best employed carefully as a part of explanatory visualizations. During the exploratory phase, you're better off using more traditional plots like the bar chart to more rapidly build your understanding of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
