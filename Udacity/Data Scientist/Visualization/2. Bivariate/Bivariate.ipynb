{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatterplots\n",
    "If we want to inspect the relationship between two numeric variables, the standard choice of plot is the scatterplot. In a scatterplot, each data point is plotted individually as a point, its x-position corresponding to one feature value and its y-position corresponding to the second. One basic way of creating a scatterplot is through Matplotlib's scatter function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data = df, x = 'num_var1', y = 'num_var2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a generally positive relationship between the two variables, as higher values of the x-axis variable are associated with greatly increasing values of the variable plotted on the y-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative Approach\n",
    "Seaborn's regplot function combines scatterplot creation with regression function fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.regplot(data = df, x = 'num_var1', y = 'num_var2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The basic function parameters, \"data\", \"x\", and \"y\" are the same for regplot as they are for matplotlib's scatter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the regression function is linear, and includes a shaded confidence region for the regression estimate. In this case, since the trend looks like a \\text{log}(y) \\propto xlog(y)∝x relationship (that is, linear increases in the value of x are associated with linear increases in the log of y), plotting the regression line on the raw units is not appropriate. If we don't care about the regression line, then we could set fit_reg = False in the regplot function call. Otherwise, if we want to plot the regression line on the observed relationship in the data, we need to transform the data, as seen in the previous lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_trans(x, inverse = False):\n",
    "    if not inverse:\n",
    "        return np.log10(x)\n",
    "    else:\n",
    "        return np.power(10, x)\n",
    "\n",
    "sb.regplot(df['num_var1'], df['num_var2'].apply(log_trans))\n",
    "tick_locs = [10, 20, 50, 100, 200, 500]\n",
    "plt.yticks(log_trans(tick_locs), tick_locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overplotting, Transparency, and Jitter\n",
    "If we have a very large number of points to plot or our numeric variables are discrete-valued, then it is possible that using a scatterplot straightforwardly will not be informative. The visualization will suffer from overplotting, where the high amount of overlap in points makes it difficult to see the actual relationship between the plotted variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data = df, x = 'disc_var1', y = 'disc_var2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot, we can infer some kind of negative relationship between the two variables, but the degree of variability in the data and strength of relationship are fairly unclear. In cases like this, we may want to employ transparency and jitter to make the scatterplot more informative. Transparency can be added to a scatter call by adding the \"alpha\" parameter set to a value between 0 (fully transparent, not visible) and 1 (fully opaque)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data = df, x = 'disc_var1', y = 'disc_var2', alpha = 1/5) #Where more points overlap, the darker the image will be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative or companion to transparency, we can also add jitter to move the position of each point slightly from its true value. This is not a direct option in matplotlib's scatter function, but is a built-in option with seaborn's regplot function. x- and y- jitter can be added independently, and won't affect the fit of any regression function, if made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.regplot(data = df, x = 'disc_var1', y = 'disc_var2', fit_reg = False,\n",
    "           x_jitter = 0.2, y_jitter = 0.2, scatter_kws = {'alpha' : 1/3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The jitter settings will cause each point to be plotted in a uniform ±0.2 range of their true values. Note that transparency has been changed to be a dictionary assigned to the \"scatter_kws\" parameter. This is necessary so that transparency is specifically associated with the scatter component of the regplot function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heat Maps\n",
    "A heat map is a 2-d version of the histogram that can be used as an alternative to a scatterplot. Like a scatterplot, the values of the two numeric variables to be plotted are placed on the plot axes. Similar to a histogram, the plotting area is divided into a grid and the number of points in each grid rectangle is added up. Since there won't be room for bar heights, counts are indicated instead by grid cell color. A heat map can be implemented with Matplotlib's hist2d function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [12, 5])\n",
    "\n",
    "# left plot: scatterplot of discrete data with jitter and transparency\n",
    "plt.subplot(1, 2, 1)\n",
    "sb.regplot(data = df, x = 'disc_var1', y = 'disc_var2', fit_reg = False,\n",
    "           x_jitter = 0.2, y_jitter = 0.2, scatter_kws = {'alpha' : 1/3})\n",
    "\n",
    "# right plot: heat map with bin edges between values\n",
    "plt.subplot(1, 2, 2)\n",
    "bins_x = np.arange(0.5, 10.5+1, 1)\n",
    "bins_y = np.arange(-0.5, 10.5+1, 1)\n",
    "plt.hist2d(data = df, x = 'disc_var1', y = 'disc_var2',\n",
    "           bins = [bins_x, bins_y])\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that since we have two variables, the \"bins\" parameter takes a list of two bin edge specifications, one for each dimension. Choosing an appropriate bin size is just as important here as it was for the univariate histogram. We add a colorbar function call to add a colorbar to the side of the plot, showing the mapping from counts to colors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Variations\n",
    "To select a different color palette, you can set the \"cmap\" parameter in hist2d. The most convenient way of doing this is to set the \"cmap\" value as a string referencing a built-in Matplotlib palette. A list of valid strings can be found on this part of the Pyplot API documentation. A further discussion of color in plots will be left to the next lesson. For now, I will just show an example of reversing the default \"viridis\" color palette, by setting cmap = 'viridis_r'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, I would like to distinguish cells with zero counts from those with non-zero counts. The \"cmin\" parameter specifies the minimum value in a cell before it will be plotted. By adding a cmin = 0.5 parameter to the hist2d call, this means that a cell will only get colored if it contains at least one point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_x = np.arange(0.5, 10.5+1, 1)\n",
    "bins_y = np.arange(-0.5, 10.5+1, 1)\n",
    "plt.hist2d(data = df, x = 'disc_var1', y = 'disc_var2',\n",
    "           bins = [bins_x, bins_y], cmap = 'viridis_r', cmin = 0.5)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a lot of data, you might want to add annotations to cells in the plot indicating the count of points in each cell. From hist2d, this requires the addition of text elements one by one, much like how text annotations were added one by one to the bar plots in the previous lesson. We can get the counts to annotate directly from what is returned by hist2d, which includes not just the plotting object, but an array of counts and two vectors of bin edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist2d returns a number of different variables, including an array of counts\n",
    "bins_x = np.arange(0.5, 10.5+1, 1)\n",
    "bins_y = np.arange(-0.5, 10.5+1, 1)\n",
    "h2d = plt.hist2d(data = df, x = 'disc_var1', y = 'disc_var2',\n",
    "               bins = [bins_x, bins_y], cmap = 'viridis_r', cmin = 0.5)\n",
    "counts = h2d[0]\n",
    "\n",
    "# loop through the cell counts and add text annotations for each\n",
    "for i in range(counts.shape[0]):\n",
    "    for j in range(counts.shape[1]):\n",
    "        c = counts[i,j]\n",
    "        if c >= 7: # increase visibility on darkest cells\n",
    "            plt.text(bins_x[i]+0.5, bins_y[j]+0.5, int(c),\n",
    "                     ha = 'center', va = 'center', color = 'white')\n",
    "        elif c > 0:\n",
    "            plt.text(bins_x[i]+0.5, bins_y[j]+0.5, int(c),\n",
    "                     ha = 'center', va = 'center', color = 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have too many cells in your heat map, then the annotations will end up being too overwhelming, too much to attend to. In cases like that, it's best to leave off the annotations and let the data and colorbar speak for themselves. You're more likely to see annotations in a categorical heat map, where there are going to be fewer cells plotted. Indeed, there is a parameter built into seaborn's heatmap function for just that, as will be seen later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Violin Plots\n",
    "There are a few ways of plotting the relationship between one quantitative and one qualitative variable, that demonstrate the data at different levels of abstraction. The violin plot is on the lower level of abstraction. For each level of the categorical variable, a distribution of the values on the numeric variable is plotted. The distribution is plotted as a kernel density estimate, something like a smoothed histogram. \n",
    "Seaborn's violinplot function can be used to create violin plots combined with box plots – we'll discuss box plots on the next page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.violinplot(data = df, x = 'cat_var', y = 'num_var')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can see that the numeric data takes on a different shape in each categorical level: Some bimodality is suggested in group Alpha, a relatively high variance is observed in Beta, and Gamma and Delta are skewed negatively and positively, respectively. You can also see that each level has been rendered in a different color, like how the plain countplot was in the previous lesson. We can set the \"color\" parameter to make each curve the same color if it is not meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside each curve, there is a black shape with a white dot inside. This is the miniature box plot mentioned above. A further discussion of box plots will be performed on the next page. If you'd like to remove the box plot, you can set the inner = None parameter in the violinplot call to simplify the look of the final visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_color = sb.color_palette()[0]\n",
    "sb.violinplot(data = df, x = 'cat_var', y = 'num_var', color = base_color,\n",
    "              inner = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Variation\n",
    "Much like how the bar chart could be rendered with horizontal bars, the violin plot can also be rendered horizontally. Seaborn is smart enough to make an appropriate inference on which orientation is requested, depending on whether \"x\" or \"y\" receives the categorical variable. But if both variables are numeric (e.g., one is discretely-valued) then the \"orient\" parameter can be used to specify the plot orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_color = sb.color_palette()[0]\n",
    "sb.violinplot(data = df, x = 'num_var', y = 'cat_var', color = base_color,\n",
    "              inner = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box Plots\n",
    "A box plot is another way of showing the relationship between a numeric variable and a categorical variable. Compared to the violin plot, the box plot leans more on summarization of the data, primarily just reporting a set of descriptive statistics for the numeric values on each categorical level. A box plot can be created using seaborn's boxplot function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [10, 5])\n",
    "base_color = sb.color_palette()[0]\n",
    "\n",
    "# left plot: violin plot\n",
    "plt.subplot(1, 2, 1)\n",
    "ax1 = sb.violinplot(data = df, x = 'cat_var', y = 'num_var', color = base_color)\n",
    "\n",
    "# right plot: box plot\n",
    "plt.subplot(1, 2, 2)\n",
    "sb.boxplot(data = df, x = 'cat_var', y = 'num_var', color = base_color)\n",
    "plt.ylim(ax1.get_ylim()) # set y-axis limits to be same as left plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the \"color\" parameter is being used here to make each box the same color. In order to provide a better comparison of the violin and box plots, a ylim expression has been added to the second plot to match the two plots' y-axis limits. The Axes object returned by violinplot is assigned to a variable, ax1 is used to programmatically obtain those limit values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inner boxes and lines in the violin plot match up with the boxes and whiskers in the box plot. In a box plot, the central line in the box indicates the median of the distribution, while the top and bottom of the box represent the third and first quartiles of the data, respectively. Thus, the height of the box is the interquartile range (IQR). From the top and bottom of the box, the whiskers indicate the range from the first or third quartiles to the minimum or maximum value in the distribution. Typically, a maximum range is set on whisker length; by default this is 1.5 times the IQR. For the Gamma level, there are points below the lower whisker that indicate individual outlier points that are more than 1.5 times the IQR below the first quartile.\n",
    "\n",
    "Comparing the two plots, the box plot is a cleaner summary of the data than the violin plot. It's easier to compare statistics between the groups with a box plot. This makes a box plot worth more consideration if you have a lot of groups to compare, or if you are building explanatory plots. You can clearly see from the box plot that the Delta group has the lowest median. On the other hand, the box plot lacks as nuanced a depiction of distributions as the violin plot: you can't see the slight bimodality present in the Alpha level values. The violin plot may be a better option for exploration, especially since seaborn's implementation also includes the box plot by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Variations\n",
    "As with violinplot, boxplot can also render horizontal box plots by setting the numeric and categorical features to the appropriate arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_color = sb.color_palette()[0]\n",
    "sb.boxplot(data = df, x = 'num_var', y = 'cat_var', color = base_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In violinplot, there is an additional option for plotting summary statistics in the violin, beyond the default mini box plot. By setting inner = 'quartile', three lines will be plotted within each violin area for the three middle quartiles. The line with thick dashes indicates the median, and the two lines with shorter dashes on either side the first and third quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_color = sb.color_palette()[0]\n",
    "sb.violinplot(data = df, x = 'cat_var', y = 'num_var', color = base_color,\n",
    "              inner = 'quartile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustered Bar Charts\n",
    "To depict the relationship between two categorical variables, we can extend the univariate bar chart seen in the previous lesson into a clustered bar chart. Like a standard bar chart, we still want to depict the count of data points in each group, but each group is now a combination of labels on two variables. So we want to organize the bars into an order that makes the plot easy to interpret. In a clustered bar chart, bars are organized into clusters based on levels of the first variable, and then bars are ordered consistently across the second variable within each cluster. This is easiest to see with an example, using seaborn's countplot function. To take the plot from univariate to bivariate, we add the second variable to be plotted under the \"hue\" argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.countplot(data = df, x = 'cat_var1', hue = 'cat_var2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The legend position in this example is a bit distracting, however. We can use an Axes method to set the legend properties on the Axes object returned from countplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9fb01334005e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcountplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'cat_var1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'cat_var2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mncol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframealpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'cat_var2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sb' is not defined"
     ]
    }
   ],
   "source": [
    "ax = sb.countplot(data = df, x = 'cat_var1', hue = 'cat_var2')\n",
    "ax.legend(loc = 8, ncol = 3, framealpha = 1, title = 'cat_var2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative Approach (Heat Map)\n",
    "One alternative way of depicting the relationship between two categorical variables is through a heat map. Heat maps were introduced earlier as the 2-d version of a histogram; here, we're using them as the 2-d version of a bar chart. The seaborn function heatmap is at home with this type of heat map implementation, but the input arguments are unlike most of the visualization functions that have been introduced in this course. Instead of providing the original dataframe, we need to summarize the counts into a matrix that will then be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_counts = df.groupby(['cat_var1', 'cat_var2']).size()\n",
    "ct_counts = ct_counts.reset_index('count')\n",
    "ct_counts = ct_counts.pivot(index = 'cat_var2', columns = 'cat_var1', values = 'count')\n",
    "sb.heatmap(ct_counts)\n",
    "sb.heatmap(ct_counts, annot = True, fmt = 'd')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "annot = True makes it so annotations show up in each cell, but the default string formatting only goes to two digits of precision. Adding fmt = 'd' means that annotations will all be formatted as integers instead. You can use fmt = '.0f' if you have any cells with no counts, in order to account for NaNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faceting\n",
    "One general visualization technique that will be useful for you to know about to handle plots of two or more variables is faceting. In faceting, the data is divided into disjoint subsets, most often by different levels of a categorical variable. For each of these subsets of the data, the same plot type is rendered on other variables. Faceting is a way of comparing distributions or relationships across levels of additional variables, especially when there are three or more variables of interest overall. While faceting is most useful in multivariate visualization, it is still valuable to introduce the technique here in our discussion of bivariate plots.\n",
    "\n",
    "For example, rather than depicting the relationship between one numeric variable and one categorical variable using a violin plot or box plot, we could use faceting to look at a histogram of the numeric variable for subsets of the data divided by categorical variable levels. Seaborn's FacetGrid class facilitates the creation of faceted plots. There are two steps involved in creating a faceted plot. First, we need to create an instance of the FacetGrid object and specify the feature we want to facet by (\"cat_var\" in our example). Then we use the map method on the FacetGrid object to specify the plot type and variable(s) that will be plotted in each subset (in this case, histogram on \"num_var\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sb.FacetGrid(data = df, col = 'cat_var')\n",
    "g.map(plt.hist, \"num_var\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the map call, just set the plotting function and variable to be plotted as positional arguments. Don't set them as keyword arguments, like x = \"num_var\", or the mapping won't work properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each subset of the data is being plotted independently. Each uses the default of ten bins from hist to bin together the data, and each plot has a different bin size. Despite that, the axis limits on each facet are the same to allow clear and direct comparisons between groups. It's still worth cleaning things a little bit more by setting the same bin edges on all facets. Extra visualization parameters can be set as additional keyword arguments to the map function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = np.arange(-3, df['num_var'].max()+1/3, 1/3)\n",
    "g = sb.FacetGrid(data = df, col = 'cat_var')\n",
    "g.map(plt.hist, \"num_var\", bins = bin_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Variation\n",
    "If you have many categorical levels to plot, then you might want to add more arguments to the FacetGrid object initialization to facilitate clarity in conveyance of information. The example below includes a categorical variable, \"many_cat_var\", that has fifteen different levels. Setting col_wrap = 5 means that the plots will be organized into rows of five facets each, rather than a single long row of fifteen plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_means = df.groupby(['many_cat_var']).mean()\n",
    "group_order = group_means.sort_values(['num_var'], ascending = False).index\n",
    "\n",
    "g = sb.FacetGrid(data = df, col = 'many_cat_var', col_wrap = 5, size = 2,\n",
    "                 col_order = group_order)\n",
    "g.map(plt.hist, 'num_var', bins = np.arange(5, 15+1, 1))\n",
    "g.set_titles('{col_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other operations may be performed to increase the immediate readability of the plots: setting each facet height to 2 inches (\"size\"), sorting the facets by group mean (\"col_order\"), limiting the number of bin edges, and changing the titles of each facet to just the categorical level name using the set_titles method and {col_name} template variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted Bar Charts\n",
    "Histograms and bar charts were introduced in the previous lesson as depicting the distribution of numeric and categorical variables, respectively, with the height (or length) of bars indicating the number of data points that fell within each bar's range of values. These plots can be adapted for use as bivariate plots by, instead of indicating count by height, indicating a mean or other statistic on a second variable.\n",
    "For example, we could plot a numeric variable against a categorical variable by adapting a bar chart so that its bar heights indicate the mean of the numeric variable. This is the purpose of seaborn's barplot function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_color = sb.color_palette()[0]\n",
    "sb.barplot(data = df, x = 'cat_var', y = 'num_var', color = base_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different hues are automatically assigned to each category level unless a fixed color is set in the \"color\" parameter, like in countplot and violinplot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar heights indicate the mean value on the numeric variable, with error bars plotted to show the uncertainty in the mean based on variance and sample size. The Delta bar dips below the 0 axis due to the negative mean.\n",
    "\n",
    "As an alternative, the pointplot function can be used to plot the averages as points rather than bars. This can be useful if having bars in reference to a 0 baseline aren't important or would be confusing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.pointplot(data = df, x = 'cat_var', y = 'num_var', linestyles = \"\")\n",
    "plt.ylabel('Avg. value of num_var')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, pointplotwill connect values by a line. This is fine if the categorical variable is ordinal in nature, but it can be a good idea to remove the line via linestyles = \"\" for nominal data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plots can be useful alternatives to the box plot and violin plot if the data is not conducive to either of those plot types. For example, if the numeric variable is binary in nature, taking values only of 0 or 1, then a box plot or violin plot will not be informative, leaving the adapted bar chart as the best choice for displaying the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [12, 5])\n",
    "base_color = sb.color_palette()[0]\n",
    "\n",
    "# left plot: violin plot\n",
    "plt.subplot(1, 3, 1)\n",
    "sb.violinplot(data = df, x = 'condition', y = 'binary_out', inner = None,\n",
    "              color = base_color)\n",
    "plt.xticks(rotation = 10) # include label rotation due to small subplot size\n",
    "\n",
    "# center plot: box plot\n",
    "plt.subplot(1, 3, 2)\n",
    "sb.boxplot(data = df, x = 'condition', y = 'binary_out', color = base_color)\n",
    "plt.xticks(rotation = 10)\n",
    "\n",
    "# right plot: adapted bar chart\n",
    "plt.subplot(1, 3, 3)\n",
    "sb.barplot(data = df, x = 'condition', y = 'binary_out', color = base_color)\n",
    "plt.xticks(rotation = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted Histograms\n",
    "Matplotlib's hist function can also be adapted so that bar heights indicate value other than a count of points through the use of the \"weights\" parameter. By default, each data point is given a weight of 1, so that the sum of point weights in each bin is equal to the number of points. If we change the weights to be a representative function of each point's value on a second variable, then the sum will end up representing something other than a count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = np.arange(0, df['num_var'].max()+1/3, 1/3)\n",
    "\n",
    "# count number of points in each bin\n",
    "bin_idxs = pd.cut(df['num_var'], bin_edges, right = False, include_lowest = True,\n",
    "                  labels = False).astype(int)\n",
    "pts_per_bin = df.groupby(bin_idxs).size()\n",
    "\n",
    "num_var_wts = df['binary_out'] / pts_per_bin[bin_idxs].values\n",
    "\n",
    "# plot the data using the calculated weights\n",
    "plt.hist(data = df, x = 'num_var', bins = bin_edges, weights = num_var_wts)\n",
    "plt.xlabel('num_var')\n",
    "plt.ylabel('mean(binary_out)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the mean of the y-variable (\"binary_out\") in each bin, the weight of each point should be equal to the y-variable value, divided by the number of points in its x-bin (num_var_wts). As part of this computation, we make use of pandas' cut function in order to associate each data point to a particular bin (bin_idxs). The labels = False parameter means that each point's bin membership is associated by a numeric index, rather than a string. We use these numeric indices into the pts_per_bin, with the .values at the end necessary in order for the Series' indices to not be confused between the indices of df['binary_out']."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line Plots\n",
    "The line plot is a fairly common plot type that is used to plot the trend of one numeric variable against values of a second variable. In contrast to a scatterplot, where all data points are plotted, in a line plot, only one point is plotted for every unique x-value or bin of x-values (like a histogram). If there are multiple observations in an x-bin, then the y-value of the point plotted in the line plot will be a summary statistic (like mean or median) of the data in the bin. The plotted points are connected with a line that emphasizes the sequential or connected nature of the x-values.\n",
    "\n",
    "If the x-variable represents time, then a line plot of the data is frequently known as a time series plot. Often, we have only one observation per time period, like in stock or currency charts. While there is a seaborn function tsplot that is intended to be used with time series data, it is fairly specialized and (as of this writing's seaborn 0.8) is slated for major changes.\n",
    "\n",
    "Instead, we will make use of Matplotlib's errorbar function, performing some processing on the data in order to get it into its necessary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(data = df, x = 'num_var1', y = 'num_var2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just blindly stick a dataframe into the function without considering its structure, we might end up with a mess like the above. The function just plots all the data points as a line, connecting values from the first row of the dataframe to the last row. In order to create the line plot as intended, we need to do additional work to summarize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set bin edges, compute centers\n",
    "bin_size = 0.25\n",
    "xbin_edges = np.arange(0.5, df['num_var1'].max()+bin_size, bin_size)\n",
    "xbin_centers = (xbin_edges + bin_size/2)[:-1]\n",
    "\n",
    "# compute statistics in each bin\n",
    "data_xbins = pd.cut(df['num_var1'], xbin_edges, right = False, include_lowest = True)\n",
    "y_means = df['num_var2'].groupby(data_xbins).mean()\n",
    "y_sems = df['num_var2'].groupby(data_xbins).sem()\n",
    "\n",
    "# plot the summarized data\n",
    "plt.errorbar(x = xbin_centers, y = y_means, yerr = y_sems)\n",
    "plt.xlabel('num_var1')\n",
    "plt.ylabel('num_var2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the x-variable ('num_var1') is continuous, we first set a number of bins into which the data will be grouped. In addition to the usual edges, the center of each bin is also computed for later plotting. For the points in each bin, we compute the mean and standard error of the mean. Note that the cut function call is simpler here than in the previous page, since we don't need to compute individual point weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting part of the above summarization of the data is that the uncertainty in the mean generally increases with increasing x-values. But for the largest two points, there are no error bars. Looking at the default errorbar plot (or the scatterplot below), we can see this is due to there only being one point in each of the last two bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternate Variations\n",
    "Instead of computing summary statistics on fixed bins, you can also make computations on a rolling window through use of pandas' rolling method. Since the rolling window will make computations on sequential rows of the dataframe, we should use sort_values to put the x-values in ascending order first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute statistics in a rolling window\n",
    "df_window = df.sort_values('num_var1').rolling(15)\n",
    "x_winmean = df_window.mean()['num_var1']\n",
    "y_median = df_window.median()['num_var2']\n",
    "y_q1 = df_window.quantile(.25)['num_var2']\n",
    "y_q3 = df_window.quantile(.75)['num_var2']\n",
    "\n",
    "# plot the summarized data\n",
    "base_color = sb.color_palette()[0]\n",
    "line_color = sb.color_palette('dark')[0]\n",
    "plt.scatter(data = df, x = 'num_var1', y = 'num_var2')\n",
    "plt.errorbar(x = x_winmean, y = y_median, c = line_color)\n",
    "plt.errorbar(x = x_winmean, y = y_q1, c = line_color, linestyle = '--')\n",
    "plt.errorbar(x = x_winmean, y = y_q3, c = line_color, linestyle = '--')\n",
    "\n",
    "plt.xlabel('num_var1')\n",
    "plt.ylabel('num_var2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we're also not limited to just one line when plotting. When multiple Matplotlib functions are called one after the other, all of them will be plotted on the same axes. Instead of plotting the mean and error bars, we will plot the three central quartiles, laid on top of the scatterplot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another bivariate application of line plots is to plot the distribution of a numeric variable for different levels of a categorical variable. This is another alternative to using violin plots, box plots, and faceted histograms. With the line plot, one line is plotted for each category level, like overlapping the histograms on top of one another. This can be accomplished through multiple errorbar calls using the methods above, or by performing multiple hist calls, setting the \"histtype = step\" parameter so that the bars are depicted as unfilled lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = np.arange(-3, df['num_var'].max()+1/3, 1/3)\n",
    "g = sb.FacetGrid(data = df, hue = 'cat_var', size = 5)\n",
    "g.map(plt.hist, \"num_var\", bins = bin_edges, histtype = 'step')\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I'm performing the multiple hist calls through the use of FacetGrid, setting the categorical variable on the \"hue\" parameter rather than the \"col\" parameter. You'll see more of this parameter of FacetGrid in the next lesson. I've also added an add_legend method call so that we can identify which level is associated with each curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the \"Alpha\" curve seems to be pretty lost behind the other three curves since the relatively low number of counts is causing a lot of overlap. Perhaps connecting the centers of the bars with a line, like what was seen in the first errorbar example, would be better.\n",
    "\n",
    "Functions you provide to the map method of FacetGrid objects do not need to be built-ins. Below, I've written a function to perform the summarization operations seen above to plot an errorbar line for each level of the categorical variable, then fed that function (freq_poly) to map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_poly(x, bins = 10, **kwargs):   # **kwargs is used to allow additional keyword arguments to be set for the errorbar function.\n",
    "    \"\"\" Custom frequency polygon / line plot code. \"\"\"\n",
    "    # set bin edges if none or int specified\n",
    "    if type(bins) == int:\n",
    "        bins = np.linspace(x.min(), x.max(), bins+1)\n",
    "    bin_centers = (bin_edges[1:] + bin_edges[:-1]) / 2\n",
    "\n",
    "    # compute counts\n",
    "    data_bins = pd.cut(x, bins, right = False,\n",
    "                       include_lowest = True)\n",
    "    counts = x.groupby(data_bins).count()\n",
    "\n",
    "    # create plot\n",
    "    plt.errorbar(x = bin_centers, y = counts, **kwargs)\n",
    "\n",
    "bin_edges = np.arange(-3, df['num_var'].max()+1/3, 1/3)\n",
    "g = sb.FacetGrid(data = df, hue = 'cat_var', size = 5)\n",
    "g.map(freq_poly, \"num_var\", bins = bin_edges)\n",
    "g.add_legend()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be cases where you are interested to see how closely your numeric data follows some hypothetical distribution. This might be important for certain parametric statistical tests, like checking for assumptions of normality. In cases like this, you can use a quantile-quantile plot, or Q-Q plot, to make a visual comparison between your data and your reference distribution. Take for example the following comparison of the following data and a hypothetical normal distribution using the sample statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a histogram of the data\n",
    "bin_size = 0.5\n",
    "bin_edges = np.arange(4, 18 + bin_size, bin_size)\n",
    "plt.hist(data = df, x = 'num_var', bins = bin_edges);\n",
    "\n",
    "# overlay a theoretical normal distribution on top\n",
    "samp_mean = df['num_var'].mean()\n",
    "samp_sd = df['num_var'].std()\n",
    "\n",
    "from scipy.stats import norm\n",
    "x = np.linspace(4, 18, 200)\n",
    "y = norm.pdf(x, samp_mean, samp_sd) # normal distribution heights\n",
    "y *= df.shape[0] * bin_size # scale the distribution height\n",
    "\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matplotlib plot function is a generic function for plotting y-values against x-values, by default a line connecting each x-y pair in sequence. In this case, I first use numpy's linspace function to generate x-values across the range of the plot. Note that the first two arguments match the bin_edges limits, while the third argument specifies the number of values to generate between the two endpoints. Then, I use the scipy package's norm class to get the height of the normal distribution curve at those x-values, using the sample mean and standard deviation as distribution parameters. pdf stands for probability density function, which returns the normal distribution height (density) at each value of x. These values are such that the total area under the curve will add up to 1. Since we've got a histogram with absolute counts on the y-axis, we need to scale the curve so it's on the same scale as the main plot: we do this by multiplying the curve heights by the number of data points and bin size. The code above gives us the following plot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a visual inspection of this overlaid plot, it looks like the data is a bit sparse on the right side compared to the expected normal distribution. There's also a bit of a spike of values between 11 and 12. On the other hand, the left side of the curve isn't too far off from the expected distribution, though it might be said that we might be missing some expected points in the left tail of the distribution. The question that we'd like to address is if there's enough evidence from what we've observed to say that the data is significantly different from the expected normal distribution.\n",
    "\n",
    "One way we could approach this is through a statistical test, such as using scipy's shapiro function to perform the Shapiro-Wilk test. But since this is a course on data visualization, we'll inspect this question visually, using the Q-Q plot type teased at the top of the page. The main idea of the plot is this: if the data was normally distributed, then we'd expect a certain pattern in terms of how far each data point is from the mean of the distribution. If we order the points from smallest to largest, then we could compare how large the _k_-th ranked data point is against the _k_-th ranked point from the expected distribution.\n",
    "\n",
    "To get these expected values, we'll make use of the norm class's ppf function, which stands for percent point function. The ppf takes as input a proportion (valued between 0 and 1) and returns the value in the distribution that would leave that proportion of the curve to the left. For a standard normal distribution (mean = 0, standard deviation = 1), the ppf(0.25) = -0.674ppf(0.25)=−0.674, ppf(0.5) = 0ppf(0.5)=0, and ppf(0.75) = 0.674ppf(0.75)=0.674. The main question, then, is what values to stick into the ppf.\n",
    "\n",
    "There's a few different conventions around this, but they generally take the form of the following equation:\n",
    "\n",
    "Given _n_ data points, the _k_-th value should be at probability point \\frac{k-a}{n+1-2a}  for some _a_ between 0 and 1 (inclusive).\n",
    "\n",
    "This equation distributes the probability points symmetrically about 0.5, and adjusting _a_ changes how much probability is left in the tails of the [0,1] range. Commonly, _a_ is set to a balanced value of 0.5, which gives the equation \\frac{k-0.5}{n} .Let's put this all together using code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = df.shape[0]\n",
    "qs = (np.arange(n_points) - .5) / n_points\n",
    "expected_vals = norm.ppf(qs, samp_mean, samp_sd)\n",
    "\n",
    "plt.scatter(expected_vals, df['num_var'].sort_values())\n",
    "plt.plot([4,18],[4,18],'--', color = 'black')\n",
    "plt.axis('equal')\n",
    "plt.xlabel('Expected Values')\n",
    "plt.ylabel('Observed Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a good idea to label the axes in this case. Since the actual and expected data are both on the same scale, the labels are a big help to keep things clear. In addition, rather than just plotting the expected and actual data alone, I've also added another plot call to add a diagonal x = y line. If the data matches the actual values perfectly on the expected value, they will fall directly on that diagonal line. The plt.axis('equal') line supports the visualization, as it will set the axis scaling to be equal, and the diagonal line will be at a 45 degree angle.Excepting the smallest and largest few points, the distribution of observed values is actually fairly in line with the distribution of expected values – that is, it falls along the diagonal line. The smallest and largest observed points are larger than the values that would be expected from the normal distribution, but it's not by much. Given how much farther values can get spread out in the tails of the normal distribution, this shouldn't be a major concern. We're probably fine in treating the data as normally distributed.\n",
    "\n",
    "Usually, the Q-Q plot is computed and rendered in terms of standardized units, rather than the scale of the original data. A standardized dataset has a mean of 0 and standard deviation of 1, so to convert a set of values into standard scores, we just need to subtract the sample mean from each value to center it around 0, then divide by the sample standard deviation to scale it. Calling methods of the norm class without arguments for the mean or standard deviation assume the standard normal distribution. The code changes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_scores = norm.ppf(qs)\n",
    "data_scores = (df['num_var'].sort_values() - samp_mean) / samp_sd\n",
    "\n",
    "plt.scatter(expected_scores, data_scores)\n",
    "plt.plot([-2.5,3],[-2.5,3],'--', color = 'black')\n",
    "plt.axis('equal')\n",
    "plt.xlabel('Expected Standard Scores')\n",
    "plt.ylabel('Observed Standard Scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the shape of the data has not changed since both datasets have been scaled in the exact same way. One of the reasons for performing this scaling is that it makes it easier to talk about the data values against the expected, theoretical distribution. In the first plot, there's no clear indication of where the center of the data lies, and how spread out the data is from that center. In the latter plot, we can use our expectations for how much of the data should be one or two standard deviations from the mean to better understand how the data is distributed. It also separates the values of the theoretical distribution from any properties of the observed data.\n",
    "\n",
    "Before closing this page out, let's take a quick look at the Q-Q plot when the data distribution does not fit the normal distribution assumptions. Instead of generating data from a normal distribution, I'll now generate data from a uniform distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the data\n",
    "np.random.seed(8322489)\n",
    "\n",
    "n_points = 120\n",
    "unif_data = np.random.uniform(0, 10, n_points)\n",
    "\n",
    "# set up the figure\n",
    "plt.figure(figsize = [12, 5])\n",
    "\n",
    "# left subplot: plot the data\n",
    "plt.subplot(1, 2, 1)\n",
    "bin_size = 0.5\n",
    "bin_edges = np.arange(0, 10 + bin_size, bin_size)\n",
    "plt.hist(x = unif_data, bins = bin_edges);\n",
    "\n",
    "# overlay a theoretical normal distribution on top\n",
    "samp_mean = unif_data.mean()\n",
    "samp_sd = unif_data.std()\n",
    "\n",
    "from scipy.stats import norm\n",
    "x = np.linspace(0, 10, 200)\n",
    "y = norm.pdf(x, samp_mean, samp_sd) # normal distribution heights\n",
    "y *= n_points * bin_size # scale the distribution height\n",
    "plt.plot(x, y)\n",
    "\n",
    "# right subplot: create a Q-Q plot\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "qs = (np.arange(n_points) - .5) / n_points\n",
    "expected_scores = norm.ppf(qs)\n",
    "data_scores = (np.sort(unif_data) - samp_mean) / samp_sd\n",
    "\n",
    "plt.scatter(expected_scores, data_scores)\n",
    "plt.plot([-2.5,2.5],[-2.5,2.5],'--', color = 'black')\n",
    "plt.axis('equal')\n",
    "plt.xlabel('Expected Standard Scores')\n",
    "plt.ylabel('Observed Standard Scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compare the random standardized scores drawn from the uniform distribution to the expected scores from the theoretical normal distribution in the Q-Q plot, we see an S-shaped curve. The comparison of values in the middle of the curve are approximately linear in trend, but the slope is steeper than the desired y = x. Meanwhile on the edges, the slope is extremely shallow, as the uniform distribution is fixed to a finite range, but the normal distribution values in the tails are expected to be much further away. You can somewhat see this in the superimposed distribution line in the left-side plot, where even at the edges of the data, there is still quite a bit of height to the theoretical normal curve. All of this contributes to the result that the randomly-generated uniform data can't be well-approximated by the normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swarm Plots\n",
    "In this lesson, you saw many ways of depicting the relationship between a numeric variable and a categorical variable. Violin plots depicted distributions as density curves, while box plots took a more summary approach, plotting the quantiles as boxes with whiskers. Another alternative to these plots is the swarm plot. Similar to a scatterplot, each data point is plotted with position according to its value on the two variables being plotted. Instead of randomly jittering points as in a normal scatterplot, points are placed as close to their actual value as possible without allowing any overlap. A swarm plot can be created in seaborn using the swarmplot function, similar to how you would a call violinplot or boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [12, 5])\n",
    "base_color = sb.color_palette()[0]\n",
    "\n",
    "# left plot: violin plot\n",
    "plt.subplot(1, 3, 1)\n",
    "ax1 = sb.violinplot(data = df, x = 'cat_var', y = 'num_var', color = base_color)\n",
    "\n",
    "# center plot: box plot\n",
    "plt.subplot(1, 3, 2)\n",
    "sb.boxplot(data = df, x = 'cat_var', y = 'num_var', color = base_color)\n",
    "plt.ylim(ax1.get_ylim()) # set y-axis limits to be same as left plot\n",
    "\n",
    "# right plot: swarm plot\n",
    "plt.subplot(1, 3, 3)\n",
    "sb.swarmplot(data = df, x = 'cat_var', y = 'num_var', color = base_color)\n",
    "plt.ylim(ax1.get_ylim()) # set y-axis limits to be same as left plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plots side by side, you can see relative pros and cons of the swarm plot. Unlike the violin plot and box plot, every point is plotted, so we can now compare the frequency of each group in the same plot. While there is some distortion due to location jitter, we also have a more concrete picture of where the points actually lie, removing the long tails that can be present in violin plots.\n",
    "\n",
    "However, it is only reasonable to use a swarm plot if we have a small or moderate amount of data. If we have too many points, then the restrictions against overlap will cause too much distortion or require a lot of space to plot the data comfortably. In addition, having too many points can actually be a distraction, making it harder to see the key signals in the visualization. Use your findings from univariate visualizations to inform which bivariate visualizations will be best, or simply experiment with different plot types to see what is most informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rug and Strip Plots\n",
    "You might encounter, or be interested in, marginal distributions that are plotted alongside bivariate plots such as scatterplots. A marginal distribution is simply the univariate distribution of a variable, ignoring the values of any other variable. For quantitative data, histograms or density curves are fine choices for marginal plot, but you might also see the rug plot employed. In a rug plot, all of the data points are plotted on a single axis, one tick mark or line for each one. Compared to a marginal histogram, the rug plot suffers somewhat in terms of readability of the distribution, but it is more compact in its representation of the data.\n",
    "\n",
    "Seaborn's JointGrid class enables this plotting of bivariate relationship with marginal univariate plots for numeric data. The plot_joint method specifies a plotting function for the main, joint plot for the two variables, while the plot_marginals method specifies the plotting function for the two marginal plots. Here, we make use of seaborn's rugplot function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sb.JointGrid(data = df, x = 'num_var1', y = 'num_var2')\n",
    "g.plot_joint(plt.scatter)\n",
    "g.plot_marginals(sb.rugplot, height = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The \"height\" parameter specifies the rug ticks to be 0.25 the height of the marginal axis size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rug plot is fine here since the data isn't particularly numerous or overly dense. In other circumstances, a histogram or density curve will be more appropriate. You probably won't consider the rug plot as a primary plot choice, but it can be a good supporter plot in certain circumstances.\n",
    "\n",
    "Another supporting plot type similar to the rug plot is the strip plot. It's like a swarm plot (see the previous page) but without any dodging or jittering to keep points separate or off the categorical line. You can also think of it as a rug plot faceted by categorical levels. You can use seaborn's swarmplot function to add a swarm plot to any other plot. The inner = \"stick\" and inner = \"point\" options can also be used with the violinplot function to include a swarm plot inside of the violin areas, instead of a box plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [10, 5])\n",
    "base_color = sb.color_palette()[0]\n",
    "\n",
    "# left plot: strip plot\n",
    "plt.subplot(1, 2, 1)\n",
    "ax1 = sb.stripplot(data = df, x = 'num_var', y = 'cat_var',\n",
    "                   color = base_color)\n",
    "\n",
    "# right plot: violin plot with inner strip plot as lines\n",
    "plt.subplot(1, 2, 2)\n",
    "sb.violinplot(data = df, x = 'num_var', y = 'cat_var', color = base_color,\n",
    "             inner = 'stick')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common plotting technique has not been discussed thus far in the course, and that’s stacking. Stacked bar charts and histograms are not uncommon, but there are often better plot choices available.\n",
    "\n",
    "The most basic stacked chart takes a single bar representing the full count, and divides it into colored segments based on frequencies on a categorical variable. If this sounds familiar, that's because it almost perfectly coincides with the description of a pie chart, except that the shape being divided is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing: count and sort by the number of instances of each category\n",
    "sorted_counts = df['cat_var'].value_counts()\n",
    "\n",
    "# establish the Figure\n",
    "plt.figure(figsize = [12, 5])\n",
    "\n",
    "# left plot: pie chart\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pie(sorted_counts, labels = sorted_counts.index, startangle = 90,\n",
    "        counterclock = False);\n",
    "plt.axis('square');\n",
    "\n",
    "# right plot: horizontally stacked bar\n",
    "plt.subplot(1, 2, 2)\n",
    "baseline = 0\n",
    "for i in range(sorted_counts.shape[0]):\n",
    "    plt.barh(y = 1, width = sorted_counts[i], left = baseline)\n",
    "    baseline += sorted_counts[i]\n",
    "\n",
    "plt.legend(sorted_counts.index)  # add a legend for labeling\n",
    "plt.ylim([0,2]) # give some vertical spacing around the bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stacked bar is built through successive calls of the matplotlib barh function; each time the function is called, the bar that is plotted is assigned a new color. The choice of \"y\" is arbitrary: it'll just center the bar around y = 1, but it doesn't have any inherent meaning. The \"left\" parameter specifies the left edge of each bar added to the stack, which starts at the baseline of 0 and is built up with each stacked bar. Note in this case that the bar is being plotted with absolute counts, rather than proportions. A discussion of absolute vs. relative frequencies will come later down the page!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this similarity, cautions regarding use of the stacked bar are fairly similar to that of the pie chart:\n",
    "\n",
    "Make sure that relative frequencies are a meaningful comparison.\n",
    "Try to limit yourself to a small number of categories, up to about five.\n",
    "Make sure that categories are arranged in a sensible order, e.g. by frequency for nominal data or by levels for ordinal data.\n",
    "Otherwise, the standard bar chart is a reliable option that should be used in most cases. Only use the pie chart or singly divided bar if there's a compelling reason to do so.\n",
    "\n",
    "The debate becomes more interesting when multiple features get involved. When should we feel free to create a stacked bar chart versus using a clustered bar chart? There are two major categories of stacked bar chart that I want to focus on here: plotting by absolute frequency and plotting by relative frequency. We'll start with code for an absolute frequency stacked chart below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat1_order = ['East', 'South', 'West', 'North']\n",
    "cat2_order = ['Type X', 'Type Y', 'Type Z', 'Type O']\n",
    "\n",
    "plt.figure(figsize = [12, 5])\n",
    "\n",
    "# left plot: clustered bar chart, absolute counts\n",
    "plt.subplot(1, 2, 1)\n",
    "sb.countplot(data = df, x = 'cat_var1', hue = 'cat_var2',\n",
    "             order = cat1_order, hue_order = cat2_order)\n",
    "plt.legend()\n",
    "\n",
    "# right plot: stacked bar chart, absolute counts\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "baselines = np.zeros(len(cat1_order))\n",
    "# for each second-variable category:\n",
    "for i in range(len(cat2_order)):\n",
    "    # isolate the counts of the first category,\n",
    "    cat2 = cat2_order[i]\n",
    "    inner_counts = df[df['cat_var2'] == cat2]['cat_var1'].value_counts()\n",
    "    # then plot those counts on top of the accumulated baseline\n",
    "    plt.bar(x = np.arange(len(cat1_order)), height = inner_counts[cat1_order],\n",
    "            bottom = baselines)\n",
    "    baselines += inner_counts[cat1_order]\n",
    "\n",
    "plt.xticks(np.arange(len(cat1_order)), cat1_order)\n",
    "plt.legend(cat2_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strategy for this plot is very similar to the single stacked bar shown above, except that we're using the standard bar with \"x\" and \"bottom\" parameters, and that baselines is a list of base heights. We want to create all of the bars for a particular secondary category at the same time so that creation of the legend has a 1:1 mapping to bar calls. You might notice below that the order of labels in the legend is the reverse of the order in which the bars are stacked. You'll see code to handle this in the relative frequency plot below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stacked bar chart plotted by absolute frequency carries one big advantage over the clustered bar chart: for the variable plotted on the x-axis, it's clear which category level has the highest frequency, in this case \"East\". The values of this variable can be interpreted just like the univariate bar chart. The disadvantage of the stacked bar chart comes with interpretation of the second, stacked variable. If you want to compare the relative counts of this second variable across levels of the first, you can really only do that for the category plotted on the baseline, which in this case is the blue one, \"Type X\". For the remaining categories, it's much harder to make the comparison of values – you can't really tell that the counts of \"Type O\" are larger in the \"South\" than the \"North\" from the stacked chart, where it's directly comparable in the clustered bar chart.\n",
    "\n",
    "Now, let's take a look at what happens when we create the stacked bar chart with relative frequencies instead, where each bar is scaled to a total height of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat1_order = ['East', 'South', 'West', 'North']\n",
    "cat2_order = ['Type X', 'Type Y', 'Type Z', 'Type O']\n",
    "\n",
    "artists = [] # for storing references to plot elements\n",
    "baselines = np.zeros(len(cat1_order))\n",
    "cat1_counts = df['cat_var1'].value_counts()\n",
    "\n",
    "# for each second-variable category:\n",
    "for i in range(len(cat2_order)):\n",
    "    # isolate the counts of the first category,\n",
    "    cat2 = cat2_order[i]\n",
    "    inner_counts = df[df['cat_var2'] == cat2]['cat_var1'].value_counts()\n",
    "    inner_props = inner_counts / cat1_counts\n",
    "    # then plot those counts on top of the accumulated baseline\n",
    "    bars = plt.bar(x = np.arange(len(cat1_order)),\n",
    "                   height = inner_props[cat1_order],\n",
    "                   bottom = baselines)\n",
    "    artists.append(bars)\n",
    "    baselines += inner_props[cat1_order]\n",
    "\n",
    "plt.xticks(np.arange(len(cat1_order)), cat1_order)\n",
    "plt.legend(reversed(artists), reversed(cat2_order), framealpha = 1,\n",
    "           bbox_to_anchor = (1, 0.5), loc = 6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main changes to this code compared to the previous plot. First of all, the cat1_counts variable has been computed to change the absolute frequencies into relative frequencies within each x-axis category. Secondly, some code has been added to reverse the order of bars in the legend. The artists variable has been added to store references to each of the bar groups added from each bar call. Then in the legend function call, we make use of the built-in Python function reversed to reverse the order in which the artists and labels are included in the legend. The additional parameters affect the positioning of the legend: setting an anchor for the legend box on the right side of the plot via \"bbox_to_anchor\", and positioning the anchor to the legend's left with \"loc = 6\".Since the bars are all the same height of 1 with a relative frequency stacked bar chart, we lose the ability to compare the absolute counts on the categorical variable plotted on the x-axis (i.e. we can't tell that \"East\" has the most counts and \"North\" the least amount). In exchange, we can now compare the relative prevalence of the stacked variable on both the first category on the bottom (\"Type X\") as well as the category on the top (\"Type O\"). We can now see that, in terms of relative frequency, \"Type X\" has a fairly consistent presence in \"South\", \"West\", and \"North\", and that \"Type O\" has its highest relative frequency in \"North\". Unfortunately, this still doesn't help us make easy comparisons about the \"Type Y\" and \"Type Z\" categories that are sandwiched in between. This major limitation is a big reason why other plot types like clustered bar or line charts are often preferable to stacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridgeline Plots\n",
    "One of the hot new visualization types from recent years is the ridgeline plot. In a nutshell, the ridgeline plot is a series of vertically faceted line plots or density curves, but with somewhat overlapping y-axes. This can be thought of as a contrast to the line plot variation seen in the \"Line Plots\" part of the lesson, where multiple lines were plotted on the same axes, with different hues. On this page, I'll walk through the creation of a ridgeline plot using some of the demonstration data shown in the \"Faceting\" page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_means = df.groupby(['many_cat_var']).mean()\n",
    "group_order = group_means.sort_values(['num_var'], ascending = False).index\n",
    "\n",
    "g = sb.FacetGrid(data = df, col = 'many_cat_var', col_wrap = 5, size = 2,\n",
    "                 col_order = group_order)\n",
    "g.map(plt.hist, 'num_var', bins = np.arange(5, 15+1, 1))\n",
    "g.set_titles('{col_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things immediately come to mind for changing the faceted histograms into a ridgeline plot. First of all, changing the form of the distribution plots from histograms to kernel density estimates (as seen in the Extras of the previous lesson) will make the overlaps a bit smoother. Second, we need to facet the levels by rows so that they're all stacked up on top of one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_means = df.groupby(['many_cat_var']).mean()\n",
    "group_order = group_means.sort_values(['num_var'], ascending = False).index\n",
    "\n",
    "g = sb.FacetGrid(data = df, row = 'many_cat_var', size = 0.75, aspect = 7,\n",
    "                 row_order = group_order)\n",
    "g.map(sb.kdeplot, 'num_var', shade = True)\n",
    "g.set_titles('{row_name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FacetGrid and set_titles change \"col\" to \"row\", also removing \"col_wrap\". The \"size\" and \"aspect\" dimensions have also been adjusted for the large vertical stacking of facets. The map function changes to kdeplot and removes \"bins\", adding the \"shade\" parameter in its place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got all of the group distributions stacked on top of one another for a uni-dimensional comparison, but the plot's still pretty tall. Next, we'll create some overlap between the individual subplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_means = df.groupby(['many_cat_var']).mean()\n",
    "group_order = group_means.sort_values(['num_var'], ascending = False).index\n",
    "\n",
    "# adjust the spacing of subplots with gridspec_kws\n",
    "g = sb.FacetGrid(data = df, row = 'many_cat_var', size = 0.5, aspect = 12,\n",
    "                 row_order = group_order, gridspec_kws = {'hspace' : -0.2})\n",
    "g.map(sb.kdeplot, 'num_var', shade = True)\n",
    "\n",
    "# remove the y-axes\n",
    "g.set(yticks=[])\n",
    "g.despine(left=True)\n",
    "\n",
    "g.set_titles('{row_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've added the \"gridspec_kws\" parameter to the FacetGrid call to adjust the arrangement of subplots in the grid through Matplotlib's GridSpec class. By setting \"hspec\" to a negative value, the subplot axes bounds will overlap vertically. The \"size\" and \"aspect\" parameters have also been adjusted. While I'm at it, I'll add some code on the FacetGrid object to remove the y-axis through the despine method and remove the ticks through the set method. They're going to start overlapping, and we don't really need them – we're mostly interested in the relative positions of the distributions rather than specific heights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual subplots now overlap, but we've still got a problem: the backgrounds of the subplots are opaque, thus obscuring all but the tops of all of the individual group distributions, with the exception of the lowest. In addition, the individual subplot titles overlap the other distributions with some ambiguity: these should be moved elsewhere in the individual plots. The revised code and plot look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_means = df.groupby(['many_cat_var']).mean()\n",
    "group_order = group_means.sort_values(['num_var'], ascending = False).index\n",
    "\n",
    "g = sb.FacetGrid(data = df, row = 'many_cat_var', size = 0.5, aspect = 12,\n",
    "                 row_order = group_order, gridspec_kws = {'hspace' : -0.2})\n",
    "g.map(sb.kdeplot, 'num_var', shade = True)\n",
    "\n",
    "g.set(yticks=[])\n",
    "g.despine(left=True)\n",
    "\n",
    "# set the transparency of each subplot to full\n",
    "g.map(lambda **kwargs: plt.gca().patch.set_alpha(0))\n",
    "\n",
    "# remove subplot titles and write in new labels\n",
    "def label_text(x, **kwargs):\n",
    "    plt.text(4, 0.02, x.iloc[0], ha = 'center', va = 'bottom')\n",
    "g.map(label_text, 'many_cat_var')\n",
    "g.set_xlabels('num_var')\n",
    "g.set_titles('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make clever use of the FacetGrid object's map function to perform the plot modifications. Previously, you've seen map used where the first argument is a plotting function, the following arguments are positional variable strings, and any additional arguments are keyword arguments for the plotting function. In actuality, you can set any function as the first argument, which will be applied to each facet. To apply the transparency using map, I set up an anonymous lambda function that gets the current Axes (gca), selects its background (patch), and sets its transparency to full.\n",
    "\n",
    "As for the second map argument, it sends a pandas Series to the function specified by the first argument. This Series is filtered to include only the column specified by the second map argument, with only the rows appropriate for each facet. In this case, I exploit the fact that the 'many_cat_var' column is filled with copies of the categorical feature string to specify the text string, with hardcoded positional values appropriate to the plot. (map also sends a few general keyword arguments like 'color' automatically to the specified function, hence the need for **kwargs to capture them despite not specifying any myself.) One downside to this approach is that the x-axis labels get replaced with 'many_cat_var' after the map call, thus requiring the addition of a set_xlabels function call to reset the string.\n",
    "\n",
    "The final ridgeline plot looks like this, where we can see the distribution of our numeric variable on each category, sorted by mean:https://seaborn.pydata.org/examples/kde_ridgeplot.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
