{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-Positional Encodings for Third Variables\n",
    "There are four major cases to consider when we want to plot three variables together:\n",
    "\n",
    "three numeric variables\n",
    "two numeric variables and one categorical variable\n",
    "one numeric variable and two categorical variables\n",
    "three categorical variables\n",
    "\n",
    "If we have at least two numeric variables, as in the first two cases, one common method for depicting the data is by using a scatterplot to encode two of the numeric variables, then using a non-positional encoding on the points to convey the value on the third variable, whether numeric or categorical. (You will see additional techniques later in the lesson that can also be applied to the other two cases, i.e. where we have at least two categorical variables.)\n",
    "\n",
    "Three main non-positional encodings stand out: shape, size, and color. For Matplotlib and Seaborn, color is the easiest of these three encodings to apply for a third variable. Color can be used to encode both qualitative and quantitative data, with different types of color palettes used for different use cases. Because of how broadly color can be used, a dedicated discussion of color and its use in Matplotlib and Seaborn will be deferred to the next page in the lesson.\n",
    "\n",
    "Encoding via Shape\n",
    "Shape is a good encoding for categorical variables, using one shape for each level of the categorical variable. Unfortunately, there is no built-in way to automatically assign different shapes in a single call of the scatter or regplot function. Instead, we need to write a loop to call our plotting function multiple times, isolating data points by categorical level and setting a different \"marker\" argument value for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_markers = [['A', 'o'],\n",
    "               ['B', 's']]\n",
    "\n",
    "for cat, marker in cat_markers:\n",
    "    df_cat = df[df['cat_var1'] == cat]\n",
    "    plt.scatter(data = df_cat, x = 'num_var1', y = 'num_var2', marker = marker)\n",
    "plt.legend(['A','B'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'o' string specifies circular markers for members of category 'A', while the 's' string specifies square markers for members of category 'B'. The legend function adds a legend to the plot, with one marker for every scatter call made. The function argument sets the labels for those points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the positional encodings in the plot, you can see that there is a modest positive relationship between the two numeric variables. Adding the categorical variable via shape encoding, we can see that points of category 'A' tend to be smaller than those of category 'B' in terms of the numeric x-variable (\"num_var1\"). Neither category seems to have an advantage in terms of variability or value for the numeric y-variable (\"num_var2\").\n",
    "\n",
    "Note that the two categories have automatically been double-encoded with different colors, in addition to the explicitly specified markers. If we wanted the points to have the same color as well, we could do that through the \"c\" parameter in scatter or \"color\" in regplot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding via Size\n",
    "Point size is a good encoding for numeric variables. Usually, we want the numeric values to be proportional to the area of the point markers; this is the default functionality of the \"s\" parameter in scatter. (You need to refer to \"s\" through a dictionary assigned to the \"scatter_kws\" parameter when working with regplot.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data = df, x = 'num_var1', y = 'num_var2', s = 'num_var3')\n",
    "\n",
    "# dummy series for adding legend\n",
    "sizes = [20, 35, 50]\n",
    "base_color = sb.color_palette()[0]\n",
    "legend_obj = []\n",
    "for s in sizes:\n",
    "    legend_obj.append(plt.scatter([], [], s = s, color = base_color))\n",
    "plt.legend(legend_obj, sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the creation of the plot is easier with size, we need to put in extra leg work to create a legend. Since, as noted above, legend will only create one legend entry per plot function call, we need to create additional dummy scatter objects with reference sizes to create the plot. The color is fixed so that all of the legend entries have the same color, and two arguments are provided to the legend function: the list of plotting objects to depict in the legend, and their labels.The size encoding for the third numeric variable (\"num_var3\") shows that its values are largest in the 'middle' of the distribution of values, and smaller on the upper and bottom edges. It is also clear that size is much less precise an encoding than position, so it is better used to make general, qualitative judgments than precise judgments.\n",
    "In the case of the example, the data was also scaled in a way that the marker sizes made sense as given. You might need to apply a scaling factor (e.g., multiplying or dividing all values by 2) or shift in order to make the size encoding interpretable. In particular, if the values in your third numeric variable include negative values, then you might want to choose a color encoding instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Warning on Combining Encodings\n",
    "It might seem plausible to combine both size and shape encodings into the same plot, to depict the trend in four variables at once. Technically, this may be true, but there are some cautions to be taken with this approach. One surface issue is that the code to depict the plot and a reasonable legend gets complicated. A more important issue is that point areas won't all be the same even with the same value, depending on the shape of the marker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [5,5])\n",
    "\n",
    "common_size = 2500\n",
    "plt.scatter([0],[1], marker = 's', s = common_size)\n",
    "plt.scatter([1],[1], marker = '^', s = common_size)\n",
    "plt.scatter([0],[0], marker = 'o', s = common_size)\n",
    "plt.scatter([1],[0], marker = 'X', s = common_size)\n",
    "\n",
    "# a little bit of aesthetic cleaning\n",
    "plt.xlim(-0.5,1.5)\n",
    "plt.xticks([])\n",
    "plt.ylim(-0.5,1.5)\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite having the same \"s\" values, the triangle, circle, and X markers all look smaller (have a smaller area) than the square. Perhaps this isn't a major concern, considering that size is better used as a qualitative measure for exploration. But it's still something to consider. A little more discussion of the number of variables that can be reasonably packed into a plot can be found later in the lesson (\"How much is too much?\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding via Color\n",
    "Color is a very common encoding for variables, for both qualitative and quantitative variables. You've already seen this employed in previous lessons where position could not be used to encode a value: color for category in a clustered bar chart, and color for count in a heat map (both as a 2-d histogram and as a 2-d bar chart). Here, we'll look at how to employ color in scatterplots, as well as discuss more about color palette choices depending on the type of data you have.\n",
    "If you have a qualitative variable, you can set different colors for different levels of a categorical variable through the \"hue\" parameter on seaborn's FacetGrid class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sb.FacetGrid(data = df, hue = 'cat_var1', size = 5)\n",
    "g.map(plt.scatter, 'num_var1', 'num_var2')\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For quantitative variables, we should not take the same approach, since FacetGrid expects any variable input for subsetting to be categorical. Instead, we can set color based on numeric value in the scatter function through the \"c\" parameter, much like how we set up marker sizes through \"s\". (Unlike with size, we don't have an easy way of setting color by numeric value through regplot due to how its \"color\" argument is set up.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data = df, x = 'num_var1', y = 'num_var2', c = 'num_var3')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Color Palettes\n",
    "Depending on the type of data you have, you may want to change the type of color palette that you use to depict your data. There are three major classes of color palette to consider: qualitative, sequential, and diverging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qualitative palettes are built for nominal-type data. This is the palette class taken by the default palette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.palplot(sb.color_palette(n_colors=9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a qualitative palette, consecutive color values are distinct so that there is no inherent ordering of levels implied. Colors in a good qualitative palette should also try and avoid drastic changes in brightness and saturation that would cause a reader to interpret one category as being more important than the others - unless that emphasis is deliberate and purposeful.\n",
    "\n",
    "For other types of data (ordinal and numeric), a choice may need to be made between a sequential scale and a diverging scale. In a sequential palette, consecutive color values should follow each other systematically. Typically, this follows a light-to-dark trend across a single or small range of hues, where light colors indicate low values and dark colors indicate high values. The default sequential color map, \"viridis\", takes the opposite approach, with dark colors indicating low values, and light values indicating high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.palplot(sb.color_palette('viridis', 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, a sequential palette will depict ordinal or numeric data just fine. However, if there is a meaningful zero or center value for the variable, you may want to consider using a diverging palette. In a diverging palette, two sequential palettes with different hues are put back to back, with a common color (usually white or gray) connecting them. One hue indicates values greater than the center point, while the other indicates values smaller than the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.palplot(sb.color_palette('vlag', 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting Color Palettes\n",
    "If you want to change the color map for your plot, the easiest way of doing so is by using one of the built-ins from Matplotlib or Seaborn. https://matplotlib.org/api/pyplot_summary.html#colors-in-matplotlib of the Matplotlib documentation has a list of strings that can be understood for color mappings. For most of your purposes, stick with the palettes noted in the top few tables as built-in for Matplotlib ('viridis', etc.) or from ColorBrewer; the remaining palettes may not be as perceptually consistent. Seaborn also adds in a number of its own palettes:\n",
    "\n",
    "Qualitative (all up to 6 colors): 'deep', 'pastel', 'dark', 'muted', 'bright', 'colorblind'\n",
    "Sequential: 'rocket' (white-orange-red-purple-black), 'mako' (mint-green-blue-purple-black)\n",
    "Diverging: 'vlag' (blue-white-red), 'icefire' (blue-black-orange)\n",
    "\n",
    "For all of these strings, appending '_r' reverses the palette, which is useful if a sequential or diverging palette is rendered counter to your expectations.\n",
    "A color palette can be set in FacetGrid through the \"palette\" parameter, and in scatter through the \"cmap\" parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sb.FacetGrid(data = df, hue = 'cat_var1', size = 5,\n",
    "                 palette = 'colorblind')\n",
    "g.map(plt.scatter, 'num_var1', 'num_var2')\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data = df, x = 'num_var1', y = 'num_var2', c = 'num_var3',\n",
    "            cmap = 'mako_r')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using a diverging color palette, you will likely need to specify the \"vmin\" and \"vmax\" parameters in order to have the neutral point in the palette meet the center point in the scale. Alternatively, solutions that create a different normalization function like \n",
    "https://stackoverflow.com/questions/20144529/shifted-colorbar-matplotlib\n",
    "can be used for finer control over the color map. Diverging color scales are common enough for the heatmap type that there is a \"center\" parameter for setting the central value. You'll see a demonstration of this later in the lesson (\"Plot Matrices\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warnings on Color\n",
    "There are a couple of things to be aware of in general, when working with color. First of all, try and consider color blindness when selecting color for your plots. You don't want your visualization to shut out the 8% of the population that have some kind of color vision deficiency. Fortunately, the built-in color palettes highlighted in the previous section should minimize these concerns. If you use a different, or custom palette, it might be worth checking your visualization's interpretability through a color blindness sim like this one.\n",
    "\n",
    "Secondly, you want to be aware of the effect of transparency and overlap on interpretability. If points of different color on a qualitative scale overlap, the result may be a third color that cannot be matched to something in the palette. If multiple points on a quantitative scale overlap, then the result may be a value that does not actually exist in the data. To be safe here, avoid or minimize transparency in plots with color. You may need to plot only a sample of your points in order to make sure that the effect of the third variable is clearly visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [5,5])\n",
    "\n",
    "# left: qualitative points\n",
    "plt.scatter(0,0.5,s = 1e4, c = sb.color_palette()[0], alpha = 0.5)\n",
    "plt.scatter(0,-0.5,s = 1e4, c = sb.color_palette()[1], alpha = 0.5)\n",
    "\n",
    "# right: quantitative points\n",
    "plt.scatter(1,0.5,s = 1e4, c = sb.color_palette('Blues')[2], alpha = 0.5)\n",
    "plt.scatter(1,-0.5,s = 1e4, c = sb.color_palette('Blues')[4], alpha = 0.5)\n",
    "\n",
    "# set axes for point overlap\n",
    "plt.xlim(-0.5,1.5)\n",
    "plt.ylim(-3.5,3.5)\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faceting for Multivariate Data\n",
    "In the previous lesson, you saw how FacetGrid could be used to subset your dataset across levels of a categorical variable, and then create one plot for each subset. Where the faceted plots demonstrated were univariate before, you can actually use any plot type, allowing you to facet bivariate plots to create a multivariate visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sb.FacetGrid(data = df, col = 'cat_var1', size = 4)\n",
    "g.map(sb.boxplot, 'cat_var2', 'num_var2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The faceted box plot suggests a slight interaction between the two categorical variables, where, in level B of \"cat_var1\", the level of \"cat_var2\" seems to be have a larger effect on the value of \"num_var2\", compared to the trend within \"cat_var1\" level A.\n",
    "\n",
    "FacetGrid also allows for faceting a variable not just by columns, but also by rows. We can set one categorical variable on each of the two facet axes for one additional method of depicting multivariate trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sb.FacetGrid(data = df, col = 'cat_var2', row = 'cat_var1', size = 2.5,\n",
    "                margin_titles = True)\n",
    "g.map(plt.scatter, 'num_var1', 'num_var2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting margin_titles = True means that instead of each facet being labeled with the combination of row and column variable, labels are placed separately on the top and right margins of the facet grid. This is a boon, since the default plot titles are usually too long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Adaptations of Bivariate Plots\n",
    "You also saw one other way of expanding univariate plots into bivariate plots in the previous lesson: substituting count on a bar chart or histogram for the mean, median, or some other statistic of a second variable. This adaptation can also be done for bivariate plots like the heat map, clustered bar chart, and line plot, to allow them to depict multivariate relationships.\n",
    "\n",
    "If we want to depict the mean of a third variable in a 2-d histogram, we need to change the weights of points in the hist2d function similar to how we changed the weights in the 1-d histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbin_edges = np.arange(0.25, df['num_var1'].max()+0.5, 0.5)\n",
    "ybin_edges = np.arange(7,    df['num_var2'].max()+0.5, 0.5)\n",
    "\n",
    "# count number of points in each bin\n",
    "xbin_idxs = pd.cut(df['num_var1'], xbin_edges, right = False,\n",
    "                    include_lowest = True, labels = False).astype(int)\n",
    "ybin_idxs = pd.cut(df['num_var2'], ybin_edges, right = False,\n",
    "                    include_lowest = True, labels = False).astype(int)\n",
    "\n",
    "pts_per_bin = df.groupby([xbin_idxs, ybin_idxs]).size()\n",
    "pts_per_bin = pts_per_bin.reset_index()\n",
    "pts_per_bin = pts_per_bin.pivot(index = 'num_var1', columns = 'num_var2').values\n",
    "\n",
    "z_wts = df['num_var3'] / pts_per_bin[xbin_idxs, ybin_idxs]\n",
    "\n",
    "# plot the data using the calculated weights\n",
    "plt.hist2d(data = df, x = 'num_var1', y = 'num_var2', weights = z_wts,\n",
    "           bins = [xbin_edges, ybin_edges], cmap = 'viridis_r', cmin = 0.5);\n",
    "plt.xlabel('num_var1')\n",
    "plt.ylabel('num_var2');\n",
    "plt.colorbar(label = 'mean(num_var3)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering how few data points there are in the example, and how cleanly the third variable is distributed, the adapted heat map is a bit excessive in terms of work. The low level of point overlap observed earlier means that the scatterplot with color or size encoding was sufficient for depicting the data. You'll be more likely to use the heat map if there is a lot of data to be aggregated.\n",
    "\n",
    "The code for the 2-d bar chart doesn't actually change much. The actual heatmap call is still the same, only the aggregation of values changes. Instead of taking size after the groupby operation, we compute the mean across dataframe columns and isolate the column of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_means = df.groupby(['cat_var1', 'cat_var2']).mean()['num_var2']\n",
    "cat_means = cat_means.reset_index(name = 'num_var2_avg')\n",
    "cat_means = cat_means.pivot(index = 'cat_var2', columns = 'cat_var1',\n",
    "                            values = 'num_var2_avg')\n",
    "sb.heatmap(cat_means, annot = True, fmt = '.3f',\n",
    "           cbar_kws = {'label' : 'mean(num_var2)'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the \"cbar_kws\" provides an additional argument to the colorbar component of the heat map call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative approach for two categorical variables and one numeric variable is to adapt a clustered bar chart using the barplot function instead of the countplot function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sb.barplot(data = df, x = 'cat_var1', y = 'num_var2', hue = 'cat_var2')\n",
    "ax.legend(loc = 8, ncol = 3, framealpha = 1, title = 'cat_var2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"hue\" parameter can also be used in a similar fashion in the boxplot, violinplot, and pointplot functions to add a categorical third variable to those plots in a clustered fashion. As a special note for pointplot, the default rendering aligns all levels of the \"hue\" categorical variable vertically. Use the \"dodge\" parameter to shift the levels in a clustered fashion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sb.pointplot(data = df, x = 'cat_var1', y = 'num_var2', hue = 'cat_var2',\n",
    "                  dodge = 0.3, linestyles = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final example, a line plot can be adapted from previous code showing how to create frequency polygons for levels of a categorical variable. In this case as well, we create a custom function to send to a FacetGrid object's map function that computes the means in each bin, then plots them as lines via errorbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_poly(x, y, bins = 10, **kwargs):\n",
    "    \"\"\" Custom adapted line plot code. \"\"\"\n",
    "    # set bin edges if none or int specified\n",
    "    if type(bins) == int:\n",
    "        bins = np.linspace(x.min(), x.max(), bins+1)\n",
    "    bin_centers = (bin_edges[1:] + bin_edges[:-1]) / 2\n",
    "\n",
    "    # compute counts\n",
    "    data_bins = pd.cut(x, bins, right = False,\n",
    "                       include_lowest = True)\n",
    "    means = y.groupby(data_bins).mean()\n",
    "\n",
    "    # create plot\n",
    "    plt.errorbar(x = bin_centers, y = means, **kwargs)\n",
    "\n",
    "bin_edges = np.arange(0.25, df['num_var1'].max()+0.5, 0.5)\n",
    "g = sb.FacetGrid(data = df, hue = 'cat_var2', size = 5)\n",
    "g.map(mean_poly, \"num_var1\", \"num_var2\", bins = bin_edges)\n",
    "g.set_ylabels('mean(num_var2)')\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Matrices\n",
    "To move back to bivariate exploration for a bit, you might come out of your initial univariate investigation of the data wanting to look at the relationship between many pairs of variables. Rather than generate these bivariate plots one by one, a preliminary option you might consider for exploration is the creation of a plot matrix. In a plot matrix, a matrix of plots is generated. Each row and column represents a different variable, and a subplot against those variables is generated in each plot matrix cell. This contrasts with faceting, where rows and columns will subset the data, and the same variables are depicted in each subplot.\n",
    "\n",
    "Seaborn's PairGrid class facilitates the creation of this kind of plot matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sb.PairGrid(data = df, vars = ['num_var1', 'num_var2', 'num_var3'])\n",
    "g.map_diag(plt.hist)\n",
    "g.map_offdiag(plt.scatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, PairGrid only expects to depict numeric variables; a typical invocation of PairGrid plots the same variables on the horizontal and vertical axes. On the diagonals, where the row and column variables match, a histogram is plotted. Off the diagonals, a scatterplot between the two variables is created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pairplot function can also be used to render this common use case in a single call.\n",
    "\n",
    "For other relationships, the flexibility of PairGrid shines. For example, if we want to look at the relationship between the numeric and categorical variables in the data, we need to set the different variable types on the rows and columns, then use an appropriate plot type for all matrix cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sb.PairGrid(data = df, x_vars = ['num_var1', 'num_var2', 'num_var3'],\n",
    "                y_vars = ['cat_var1','cat_var2'])\n",
    "g.map(sb.violinplot, inner = 'quartile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you choose to create a plot matrix, be aware that the time it takes to render the plot depends on the number of data points you have and the number of variables you want to plot. Increasing the number of variables increases the number of plots that need to be rendered in a quadratic fashion. In addition, increasing the number of variables means that the individual subplot size needs to be reduced in order to fit the matrix width on your screen. That means that, if you have a lot of data, it might be difficult to see the relationships between variables due to overplotting, and it will take a long time to complete. One recommended approach is to take a random subset of the data to plot in the plot matrix instead. Use the plot matrix to identify interesting variable pairs, and then follow it up with individual plots on the full data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrices\n",
    "For numeric variables, it can be useful to create a correlation matrix as part of your exploration. While it's true that the .corr function is perfectly fine for computing and returning a matrix of correlation coefficients, it's not too much trouble to plot the matrix as a heat map to make it easier to see the strength of the relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.heatmap(df.corr(), annot = True, fmt = '.2f', cmap = 'vlag_r', center = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the default sequential color map, a diverging color map is specified and its center is set to 0. That way, we can use hue to tell if a correlation is positive or negative, and see its strength from the hue's intensity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget that correlations are computed based on strength of linear relationship. Compare the correlation between \"num_var2\" and \"num_var3\" to the corresponding cell in the first plot matrix as an example of how a correlation statistic might be misleading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How Much is Too Much?\n",
    "By now, you've seen a lot of ways of expanding the number of variables that can be depicted in a single visualization. The basic positional axes can handle two variables: one on the horizontal and one on the vertical. You can facet by both columns and rows to add up to two variables. Encodings in shape, size, and color could add as many as three more. However, you should try and resist the temptation to overuse your newfound power, and to instead practice some restraint in the number of variables that you include in any one plot.\n",
    "\n",
    "When you have only two variables plotted, A and B, you have only one relationship to understand. Add in a third variable C, and you have three pairwise relationships: A vs. B, A vs. C, and B vs. C. You also need to consider one interaction effect between all three variables: Does the value of C affect the relationship between A and B? With a fourth variable, you have six possible pairwise relationships and four different three-variable interaction effects. This exponential explosion of possible relationships with the number of variables means that there is a potential for cognitive overload if the data isn't conveyed clearly.\n",
    "\n",
    "This is why it is so important to approach data exploration systematically, rather than just throw as many variables together as possible immediately. When you move from univariate visualizations to bivariate visualizations, you augment your previous understanding of individual distributions by seeing how they relate to one another. If you look at pairwise visualizations before putting together a trivariate plot, then you will have a clear view to how the interaction, if present, changes your previous understanding of the marginal pairwise relationship.\n",
    "\n",
    "When you move on to explanatory data visualizations, try to limit the number of variables that are introduced at the same time and make sure that the encoding choices convey the main findings to your reader in the clearest way possible. While it's good to keep a soft limit of about three or four variables in a single visualization, you can exceed this if the trends are clear or you introduce features to your reader in a systematic way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
